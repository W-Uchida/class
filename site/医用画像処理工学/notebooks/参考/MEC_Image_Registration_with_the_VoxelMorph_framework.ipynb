{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n6bCKmBVEU9F"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joVczQLTPXMZ"
      },
      "source": [
        "# MEC Entry: Introduction to image registration with VoxelMorph\n",
        "Adrian Dalca and Andrew Hoopes    \n",
        "\n",
        "This tutorial covers unsupervised learning-based registration, using the [VoxelMorph](https://github.com/voxelmorph/voxelmorph) framework.\n",
        "\n",
        "### Outline\n",
        "- **Core concepts with MNIST**   \n",
        "We will first learn to deal with data, building a model, training, registration and generalization\n",
        "- **More realistic complexity: Brain MRI (2D slices)**  \n",
        "We will then show how these models work for 2d slices of brain scans, presenting a more complex scenario    \n",
        "- **Realistic 3D Brain MRI**  \n",
        "We will illustrate full 3D registration\n",
        "- **Bonus: Building Templates (Atlases)**\n",
        "\n",
        "This tutorial is focused on tensorflow, but pytorch code covering most utilities is available as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF2zhVeiPXMa"
      },
      "source": [
        "Throughout this tutorial we assume that the images have been rigidly aligned in a (roughly) similar space.  \n",
        "Rigid alignment is also possible with VoxelMorph, but it's not our focus here.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp-Br9ntPXMc"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCw0avYgPXMd"
      },
      "source": [
        "# Preamble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW6eVBeKk2Ff"
      },
      "source": [
        "## Setup of environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElmUi5B-VObi",
        "outputId": "dfb17aef-b4f3-4a50-d67b-dcc11b402664"
      },
      "source": [
        "# unfortunately, google colab recently switched to default tensorflow 2.5.0,\n",
        "# which gives peculiar errors at times.\n",
        "# We'll go back to 2.4.1 for this tutorial.\n",
        "\n",
        "# This should take a few seconds. If for some reason this fails, feel free to ignore this cell\n",
        "# and move on. Most cells should work fine.\n",
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.4.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkhDsBkDQKQ_"
      },
      "source": [
        "# install voxelmorph, which will also install dependencies: neurite and pystrum\n",
        "!pip install voxelmorph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohNC6AESPXMe"
      },
      "source": [
        "We'll start with some common imports  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGSrH6W-PXMf"
      },
      "source": [
        "# imports\n",
        "import os, sys\n",
        "\n",
        "# third party imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2.'), 'This tutorial assumes Tensorflow 2.0+'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov7CZWsZPXMk"
      },
      "source": [
        "Next, we import two packages that will help us   \n",
        "- [voxelmorph](http://voxelmorph.mit.edu) is a deep-learning based registration library  \n",
        "- [neurite](https://github.com/adalca/neurite) is a library for medical image analysis with tensorflow  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGqLlJuIPXMl"
      },
      "source": [
        "# local imports\n",
        "import voxelmorph as vxm\n",
        "import neurite as ne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctg1luzvPXMz"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPA4UHD5PXM0"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhpEO8jtPXM1"
      },
      "source": [
        "We're going to start by registering 2D MNIST digits, and then move on to medical data later. If the data is small (like 2D MNIST), you can often load it in memory, which enables for faster training and testing. If the data is large (large 3D scans), we will likely need to load the scans on demand. More on this later.\n",
        "\n",
        "First, we're going to **load the data**. Luckily, MNIST comes with the keras framework, so we can just load it here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVZhsUDKPXM4"
      },
      "source": [
        "# You should most often have this import together with all other imports at the top,\n",
        "# but we include here here explicitly to show where data comes from\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syWMeeIYPXM8"
      },
      "source": [
        "# load MNIST data.\n",
        "# `mnist.load_data()` already splits our data into train and test.\n",
        "(x_train_load, y_train_load), (x_test_load, y_test_load) = mnist.load_data()\n",
        "\n",
        "digit_sel = 5\n",
        "\n",
        "# extract only instances of the digit 5\n",
        "x_train = x_train_load[y_train_load==digit_sel, ...]\n",
        "y_train = y_train_load[y_train_load==digit_sel]\n",
        "x_test = x_test_load[y_test_load==digit_sel, ...]\n",
        "y_test = y_test_load[y_test_load==digit_sel]\n",
        "\n",
        "# let's get some shapes to understand what we loaded.\n",
        "print('shape of x_train: {}, y_train: {}'.format(x_train.shape, y_train.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcsKjsbpPXNB"
      },
      "source": [
        "**ML detour**: separating your data in *only* train/test **often leads to problems**   \n",
        "You wouldn't want to iteratively (A) build a model, (B) train on training data, and (C) test on test data  \n",
        "Doing so will **overfit to you test set** (because you will have adapted your algorithm to your test data). It's a common mistakes in ML submissions.  \n",
        "\n",
        "We will split the 'training' into 'train/validate' data, and keep the test set for later  \n",
        "And will only look at the test data at the very end (once we're ready to submit the paper!)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwowsCuwPXNB"
      },
      "source": [
        "nb_val = 1000  # keep 1,000 subjects for validation\n",
        "x_val = x_train[-nb_val:, ...]  # this indexing means \"the last nb_val entries\" of the zeroth axis\n",
        "y_val = y_train[-nb_val:]\n",
        "x_train = x_train[:-nb_val, ...]\n",
        "y_train = y_train[:-nb_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVYdKiy-PXNE"
      },
      "source": [
        "### Visualize Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijzGwV5PXNF"
      },
      "source": [
        "When we are done loading, it's always great to visualize the data  \n",
        "Here, we use some tools from a package called `neurite`, which uses matplotlib  \n",
        "You could use matplotlib as well directly, but it would just be a bit messier  \n",
        "and here we want to illustrate the main concepts.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da2-67A9PXNH"
      },
      "source": [
        "nb_vis = 5\n",
        "\n",
        "# choose nb_vis sample indexes\n",
        "idx = np.random.choice(x_train.shape[0], nb_vis, replace=False)\n",
        "example_digits = [f for f in x_train[idx, ...]]\n",
        "\n",
        "# plot\n",
        "ne.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwoyo4BlPXNJ"
      },
      "source": [
        "Looks good!  \n",
        "\n",
        "However, luckily we included a **colorbar**, which shows us that the data is in [0, 255].  \n",
        "In neural networks it's often great to work in the ranges of [0, 1] or [-1, 1] or around there.  \n",
        "Let's fix this.\n",
        "\n",
        "In general, you should always plot your data with colorbars, which helps you catch issues before training  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMf1sxDEPXNK"
      },
      "source": [
        "# fix data\n",
        "x_train = x_train.astype('float')/255\n",
        "x_val = x_val.astype('float')/255\n",
        "x_test = x_test.astype('float')/255\n",
        "\n",
        "# verify\n",
        "print('training maximum value', x_train.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spLcvLDZPXNM"
      },
      "source": [
        "# re-visualize\n",
        "example_digits = [f for f in x_train[idx, ...]]\n",
        "ne.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CKdCLrlPXNQ"
      },
      "source": [
        "One last change. Later on, we'll see that many popular models like to have inputs that are sized as multiples of 2^N for N being the number of layers. Here, we force our images to be size 32 (2x 2^4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hglJSxeePXNQ"
      },
      "source": [
        "pad_amount = ((0, 0), (2,2), (2,2))\n",
        "\n",
        "# fix data\n",
        "x_train = np.pad(x_train, pad_amount, 'constant')\n",
        "x_val = np.pad(x_val, pad_amount, 'constant')\n",
        "x_test = np.pad(x_test, pad_amount, 'constant')\n",
        "\n",
        "# verify\n",
        "print('shape of training data', x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHR-mKAAPXNU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyizQDoQXaFP"
      },
      "source": [
        "# Unsupervised image Registration (VoxelMorph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlt2Rf1rXe1M"
      },
      "source": [
        "To perform unsupervised registration with `voxelmorph`, we'll need two things: a CNN model (which takes as input two images and outputs what we care about, the deformation field), and a loss function that enable training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBYK3jMOPXNV"
      },
      "source": [
        "## CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esYTQoPLPXNV"
      },
      "source": [
        "Given two images (which we call *moving* and *fixed*), our goal is to find the deformation between them. In learning-based methods, we use a network that takes in two images $m$ (\"moving\") and $f$ (\"fixed\") (e.g. MNIST digits of size 32x32) and outputs a dense deformation $\\phi$ (e.g. size 32x32x2, because at each pixel we want a vector telling us where to go). Intuitively, this deformation $\\phi$ gives us the correspondances between the images, and tells us how to moving the moving image to match up with the fixed image.\n",
        "\n",
        "**Note**: Registration also includes (or refers to) affine transforms, but we ignore that in this tutorial.\n",
        "\n",
        "The [VoxelMorph](http://voxelmorph.mit.edu) library provides a `VxmDense` model class for building dense deformation networks. We will discuss this class later on, but for tutorial sake, we'll start by building this model from scratch in order to demonstrate the individual components of the network. First, we're going to abstract a UNet architecture with the `vxm.networks.Unet()` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZw3dTg4PXNW"
      },
      "source": [
        "# configure unet input shape (concatenation of moving and fixed images)\n",
        "ndim = 2\n",
        "unet_input_features = 2\n",
        "inshape = (*x_train.shape[1:], unet_input_features)\n",
        "\n",
        "# configure unet features\n",
        "nb_features = [\n",
        "    [32, 32, 32, 32],         # encoder arm features\n",
        "    [32, 32, 32, 32, 32, 16]  # decoder arm features\n",
        "]\n",
        "\n",
        "# build model\n",
        "unet = vxm.networks.Unet(inshape=inshape, nb_features=nb_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njbXsd49PXNb"
      },
      "source": [
        "Let's explore the model bit..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tkv40i9PXNb"
      },
      "source": [
        "print('input shape: ', unet.input.shape)\n",
        "print('output shape:', unet.output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GubrQS5PXNf"
      },
      "source": [
        "Now we need to make sure the final output has 2 features, representing the deformation at each voxel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhZ92bohPXNf"
      },
      "source": [
        "# transform the results into a flow field.\n",
        "disp_tensor = tf.keras.layers.Conv2D(ndim, kernel_size=3, padding='same', name='disp')(unet.output)\n",
        "\n",
        "# check tensor shape\n",
        "print('displacement tensor:', disp_tensor.shape)\n",
        "\n",
        "# using keras, we can easily form new models via tensor pointers\n",
        "def_model = tf.keras.models.Model(unet.inputs, disp_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAhgaZM0ou0h"
      },
      "source": [
        "The deformation model `def_model` will now *share layers* with the UNet model, so if we change layer weights in `UNet`, they change in `def_model`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgoRqis6PXNl"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDzrcDyTPXNm"
      },
      "source": [
        "Given that the displacement $\\phi$ is output from the network,  \n",
        "we need to figure out a loss to tell if it makes sense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PqAeGyBPXNm"
      },
      "source": [
        "In a **supervised setting** we would have ground truth deformations $\\phi_{gt}$,  \n",
        "and we could use a supervised loss like MSE $= \\| \\phi - \\phi_{gt} \\|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRM_mMHEPXNo"
      },
      "source": [
        "The main idea in **unsupervised registration** is to use a loss inspired by classical registration  \n",
        "\n",
        "Without supervision, how do we know this deformation is good?  \n",
        "(1) make sure that $m \\circ \\phi$ ($m$ warped by $\\phi$) is close to $f$ for some notion of *similarity*    \n",
        "(2) regularize $\\phi$ (often meaning make sure it's smooth)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02UG_abtPXNo"
      },
      "source": [
        "To achieve (1), we need to *warp* input image $m$. To do this, we use a spatial transformation network layer, which essentially does linear interpolation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw6dKBjBPXNp"
      },
      "source": [
        "# build transformer layer\n",
        "spatial_transformer = vxm.layers.SpatialTransformer(name='transformer')\n",
        "\n",
        "# extract the first frame (i.e. the \"moving\" image) from unet input tensor\n",
        "moving_image = unet.input[..., 0, tf.newaxis]\n",
        "\n",
        "# warp the moving image with the transformer\n",
        "moved_image_tensor = spatial_transformer([moving_image, disp_tensor])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA3Xh3M1PXNq"
      },
      "source": [
        "To make sure the moved image is close to the fixed image, and to achieve smoothness loss of $\\phi$ in (2), we could put these tensors into a loss. However, it's often convenient to make these two tensors as outputs from the full model, as then we could further use the model and obtain both the moved image and the deformation field during testing in one shot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_0VMQL6PXNr"
      },
      "source": [
        "outputs = [moved_image_tensor, disp_tensor]\n",
        "vxm_model = tf.keras.models.Model(inputs=unet.inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbuUPdBfPf_4"
      },
      "source": [
        "The model we just created represents the standard, dense VoxelMorph archetecture, with a UNet component, displacement field, and final spatial transformer layer. However, you don't have to build this model from scratch every time - the VoxelMorph library provides a highly customizable `VxmDense` model class that encompasses this archetecture.\n",
        "\n",
        "From now on in the tutorial, we will be using `VxmDense` class to build models, so let's rebuild the equivolent network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWxc3w9rR7J9"
      },
      "source": [
        "# build model using VxmDense\n",
        "inshape = x_train.shape[1:]\n",
        "vxm_model = vxm.networks.VxmDense(inshape, nb_features, int_steps=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "025S9BG5Sawf"
      },
      "source": [
        "The `int_steps=0` option disables diffeomorphism, which will be covered in a more advanced step of the tutorial.\n",
        "\n",
        "The `VxmDense` model is automatically configured with two input tensors (moving and fixed inputs) instead of one. The input shape parameter should not contain any feature information, which can be set via the optional `src_feats` and `trg_feats` parameters.\n",
        "\n",
        "Let's take one final look at the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr25iVnQVjXt"
      },
      "source": [
        "print('input shape: ', ', '.join([str(t.shape) for t in vxm_model.inputs]))\n",
        "print('output shape:', ', '.join([str(t.shape) for t in vxm_model.outputs]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8NI7uvqPXNs"
      },
      "source": [
        "Now that we've learned how to easily build our networks, let's define the actual loss. The way keras works, we need to define a loss term for each output -- which matches the two losses in unsupervised registration: image matching and regularization.  \n",
        "\n",
        "The first loss is easy, it's a similarity like MSE between the warped image $m \\circ \\phi$. The `voxelmorph` library has a variety of custom loss classes.\n",
        "\n",
        "For the second, we will use a spatial gradient of the displacement.  \n",
        "We won't code this from scratch here, but we'll use the `voxelmorph` implementation.\n",
        "\n",
        "We also have to balance the loss terms with a hyperparameter. We'll have more insight on this in the advanced section via a method called [HyperMorph](https://arxiv.org/abs/2101.01035)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAKWTBVdPXNs"
      },
      "source": [
        "# loss terms\n",
        "losses = [vxm.losses.MSE().loss, vxm.losses.Grad('l2').loss]\n",
        "\n",
        "# usually, we have to balance the two losses by a hyper-parameter\n",
        "lambda_param = 0.05\n",
        "loss_weights = [1, lambda_param]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t58CwfQPXNu"
      },
      "source": [
        "Finally, we can compile the model.\n",
        "This sets up the model for training, by associating the model with a loss and an optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkYRqRO1PXNv"
      },
      "source": [
        "vxm_model.compile(optimizer='Adam', loss=losses, loss_weights=loss_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvBaf6puPXNx"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZVS-y__PXNx"
      },
      "source": [
        "To train, we need to make sure the data is in the right format and fed to the model the way we want it.  \n",
        "\n",
        "keras models can be trained with `model.fit`, which works if all data is in a big array or through a python generator that gives you batches of data.\n",
        "\n",
        "Let's code a simple data generator based on the MNIST data, which will convey the core ideas of how the data is fed at every iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbc-O66HPXNy"
      },
      "source": [
        "def vxm_data_generator(x_data, batch_size=32):\n",
        "    \"\"\"\n",
        "    Generator that takes in data of size [N, H, W], and yields data for\n",
        "    our custom vxm model. Note that we need to provide numpy data for each\n",
        "    input, and each output.\n",
        "\n",
        "    inputs:  moving [bs, H, W, 1], fixed image [bs, H, W, 1]\n",
        "    outputs: moved image [bs, H, W, 1], zero-gradient [bs, H, W, 2]\n",
        "    \"\"\"\n",
        "\n",
        "    # preliminary sizing\n",
        "    vol_shape = x_data.shape[1:] # extract data shape\n",
        "    nb_items = x_data.shape[0]\n",
        "    ndims = len(vol_shape)\n",
        "\n",
        "    # prepare a zero array the size of the deformation\n",
        "    # we'll explain this below\n",
        "    zero_phi = np.zeros([batch_size, *vol_shape, ndims])\n",
        "\n",
        "    while True:\n",
        "        # prepare inputs:\n",
        "        # images need to be of the size [batch_size, H, W, 1]\n",
        "        idx1 = np.random.randint(0, nb_items, size=batch_size)\n",
        "        moving_images = x_data[idx1, ..., np.newaxis]\n",
        "        idx2 = np.random.randint(0, nb_items, size=batch_size)\n",
        "        fixed_images = x_data[idx2, ..., np.newaxis]\n",
        "        inputs = [moving_images, fixed_images]\n",
        "\n",
        "        # prepare outputs (the 'true' moved image):\n",
        "        # of course, we don't have this, but we know we want to compare\n",
        "        # the resulting moved image with the fixed image, so we return\n",
        "        # the fixed image.\n",
        "        # Bescause keras requires a 'true' item for each output, we\n",
        "        # return a volume of zeros -- this won't be used in the end.\n",
        "        outputs = [fixed_images, zero_phi]\n",
        "\n",
        "        yield (inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJcCEElPPXN2",
        "scrolled": true
      },
      "source": [
        "# let's test it\n",
        "train_generator = vxm_data_generator(x_train)\n",
        "in_sample, out_sample = next(train_generator)\n",
        "\n",
        "# visualize\n",
        "images = [img[0, :, :, 0] for img in in_sample + out_sample]\n",
        "titles = ['moving', 'fixed', '\"moved ground-truth\" (fixed)', 'zeros']\n",
        "ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSZy_kgNh-2H"
      },
      "source": [
        "Finally, we're going to train the model. In this tutorial the training speed will depend on the CPU/GPU/TPU you get from google, so we'll just train a few iterations for a few steps each.\n",
        "\n",
        "Normally, we would train to convergence -- that is, unless the registration loss doesn't improve in a number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRJrD7E_PXN4",
        "scrolled": true
      },
      "source": [
        "# finally, let's train a model\n",
        "nb_epochs = 10\n",
        "steps_per_epoch = 100\n",
        "hist = vxm_model.fit(train_generator, epochs=nb_epochs, steps_per_epoch=steps_per_epoch, verbose=2);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W61OxZKTPXN6"
      },
      "source": [
        "It's always a good idea to visualize the loss, not just read off the numbers. This will give us a better idea of whether it's converged, etc. Tensorflow offers a powerful interactive system for visualizing called tensorboard. For this short tutorial, we will simply plot the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qaOE18CPXN7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(hist, loss_name='loss'):\n",
        "    # Simple function to plot training history.\n",
        "    plt.figure()\n",
        "    plt.plot(hist.epoch, hist.history[loss_name], '.-')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPNkbVC7PXN9"
      },
      "source": [
        "Clearly, this is not converged, and you should run it to convergence, but for the purposes of this tutorial, we'll move on.\n",
        "\n",
        "**Detour**: always zoom in your loss to the last few iterations/epochs -- looking at the entire graph can be deceiving, since the starting point makes the final epochs appear flat, but often they are not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owpa6G64PXN-"
      },
      "source": [
        "## Registration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kGpArXrPXN-"
      },
      "source": [
        "With pair-wise optimization methods (like most classical methods), to register a new pair you would need to optimize a deformation field.  \n",
        "\n",
        "With learning based registration, we simply evaluate the network for a new input pair  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaE_ZCCrPXN_"
      },
      "source": [
        "# let's get some data\n",
        "val_generator = vxm_data_generator(x_val, batch_size=1)\n",
        "val_input, _ = next(val_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKTzOvE4PXOB"
      },
      "source": [
        "*Registration*: `predict()` essentially executes the network given an input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWahG2b0PXOB"
      },
      "source": [
        "val_pred = vxm_model.predict(val_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMDio8GnPXOG"
      },
      "source": [
        "and that's it!\n",
        "\n",
        "Even though this is on MNIST only, let's see how long this takes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdev-ABqPXOG"
      },
      "source": [
        "# %timeit is a 'jupyter magic' that times the given line over several runs\n",
        "%timeit vxm_model.predict(val_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q36CJ2ruPXOJ"
      },
      "source": [
        "This is quite fast, even for MNIST. Let's visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ef2sQhrPXOK"
      },
      "source": [
        "# visualize\n",
        "images = [img[0, :, :, 0] for img in val_input + val_pred]\n",
        "titles = ['moving', 'fixed', 'moved', 'flow (x-dim)']\n",
        "ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbK2KxZfPXON"
      },
      "source": [
        "Let's visualize the flow a bit better, we'll draw little arrows.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au4CSyfWPXOO"
      },
      "source": [
        "# flow is a plotting functionality in `neurite`\n",
        "ne.plot.flow([val_pred[1].squeeze()], width=5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBsXU7dcPXOR"
      },
      "source": [
        "## Generalization\n",
        "How do learning-based methods generalize beyond training distribution ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RpOpAq8PXOR"
      },
      "source": [
        "An important caveat to learning-based registration is that they will, in general, only register samples from the distribution they've been trained from.\n",
        "\n",
        "So, what happens if we register two 7's?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_82Qh5_kPXOS"
      },
      "source": [
        "# extract only instances of the digit 7\n",
        "x_sevens = x_train_load[y_train_load==7, ...].astype('float') / 255\n",
        "x_sevens = np.pad(x_sevens, pad_amount, 'constant')\n",
        "\n",
        "# predict\n",
        "seven_generator = vxm_data_generator(x_sevens, batch_size=1)\n",
        "seven_sample, _ = next(seven_generator)\n",
        "seven_pred = vxm_model.predict(seven_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2McI6abBPXOV"
      },
      "source": [
        "# visualize\n",
        "images = [img[0, :, :, 0] for img in seven_sample + seven_pred]\n",
        "titles = ['moving', 'fixed', 'moved', 'flow']\n",
        "ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICJKDCyfPXOX"
      },
      "source": [
        "Interesting - it still works! So it **generalized beyond what we expected**. Why?  \n",
        "\n",
        "Locally, parts of the 7s look similar to the 5s, so the  registration algorithm still tries to match local neighborhoods.\n",
        "\n",
        "Let's try a different variation. What if we just modify the (original) set, but multiplied the intensities by a factor?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVmQnMGNPXOY"
      },
      "source": [
        "factor = 5\n",
        "val_pred = vxm_model.predict([f * factor for f in val_input])\n",
        "\n",
        "# visualizeb\n",
        "images = [img[0, :, :, 0] for img in val_input + val_pred]\n",
        "titles = ['moving', 'fixed', 'moved', 'flow']\n",
        "ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrlTj2j-PXOa"
      },
      "source": [
        "This broke down! Why? In this case, the network has never seen even parts of this image (because the intensities were so large). In that case, we don't know what the network will do -- it might work, it might not.\n",
        "\n",
        "Understanding when the network generalizes and when it does not is very important, and still a part of active research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zfnae-bPXOc"
      },
      "source": [
        "# Unsupervised Registration of Brain MRI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_QKRpS7PXOc"
      },
      "source": [
        "We will now register slightly more realistic data - MRIs of the brain.  \n",
        "\n",
        "To be able to train and easily register during this tutorial, we will first extract the middle slice of brain scans. Because this task does not capture deformations in the third dimensions, certain correspondances are not exactly possible.  Nonetheless, this exercise will illustrate registration with more realistic complex images.   \n",
        "\n",
        "The brain scans have been intensity-normalized, affinely aligned, and skull-stripped with FreeSurfer, to enable focusing on deformable registration. These steps may not be strictly necessary, but help in the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0cSrEOilMVl"
      },
      "source": [
        "## Data: Brain Scans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-8J-ebBlxVz"
      },
      "source": [
        "# download MRI tutorial data\n",
        "!wget https://surfer.nmr.mgh.harvard.edu/pub/data/voxelmorph/tutorial_data.tar.gz -O data.tar.gz\n",
        "!tar -xzvf data.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OvnL4xQPXOd"
      },
      "source": [
        "# we're going to load some data that we packaged for this tutorial\n",
        "npz = np.load('tutorial_data.npz')\n",
        "x_train = npz['train']\n",
        "x_val = npz['validate']\n",
        "\n",
        "# the 208 volumes are of size 192 x 160\n",
        "print('train shape:', x_train.shape)\n",
        "vol_shape = x_train.shape[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSBNCv3ZPXOg"
      },
      "source": [
        "### Data Visualization\n",
        "Let's take a look at some of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mz97MblPXOi"
      },
      "source": [
        "# extract some brains\n",
        "nb_vis = 5\n",
        "idx = np.random.randint(0, x_train.shape[0], [5,])\n",
        "example_digits = [f for f in x_train[idx, ...]]\n",
        "\n",
        "# visualize\n",
        "ne.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5SzyH_ZPXOv"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqNiSHbJPXOv"
      },
      "source": [
        "As with MNIST, let's create a standard VoxelMorph model trained with MSE and spatial smoothing losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNkaW3rLPXOw"
      },
      "source": [
        "# unet\n",
        "vxm_model = vxm.networks.VxmDense(vol_shape, nb_features, int_steps=0)\n",
        "\n",
        "# losses and loss weights\n",
        "losses = ['mse', vxm.losses.Grad('l2').loss]\n",
        "loss_weights = [1, 0.01]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YDxjtMCPXO5"
      },
      "source": [
        "From experimentation, we have found the Adam optimizer learning rate of `1e-4` performs better than `1e-3` for this problem and for small batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk3kxOc7PXO5"
      },
      "source": [
        "vxm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=losses, loss_weights=loss_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ocOoErfPXO7"
      },
      "source": [
        "Luckily, we can use the same **data generator** as before, since the only thing that's changes is the stack of data, which we pass to the generator.\n",
        "\n",
        "Let's test it first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7qZ-GGQPXO8"
      },
      "source": [
        "# get new generator\n",
        "train_generator = vxm_data_generator(x_train, batch_size=8)\n",
        "in_sample, out_sample = next(train_generator)\n",
        "\n",
        "# visualize\n",
        "images = [img[0, :, :, 0] for img in in_sample + out_sample]\n",
        "titles = ['moving', 'fixed', '\"moved ground-truth\" (fixed)', 'zeros']\n",
        "ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBiebhXEPXPB"
      },
      "source": [
        "Looks good, time to **train the model**  \n",
        "As before, we'll only train for a bit for illustration purposes. You should train to completion!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PyZ-RYKPXPB"
      },
      "source": [
        "nb_epochs = 5\n",
        "steps_per_epoch = 20\n",
        "hist = vxm_model.fit(train_generator, epochs=nb_epochs, steps_per_epoch=steps_per_epoch, verbose=2);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxZ7IadCPXPL"
      },
      "source": [
        "# as before, let's visualize what happened\n",
        "plot_history(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qHKnWC5peXD"
      },
      "source": [
        "We ran very few epochs for the purpose of the tutorial. Of course, we do not expect any sort of reasonable convergence here.\n",
        "\n",
        "To be able to move on with the tutorial without waiting too long, let's load a model we've pretrained for 200 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnh76ZL3PXPI"
      },
      "source": [
        "# load pretrained model weights\n",
        "vxm_model.load_weights('brain_2d_smooth.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYcYM4XoPXPN"
      },
      "source": [
        "Now let's see some results using validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvmVFKcpPXPO"
      },
      "source": [
        "# create the validation data generator\n",
        "val_generator = vxm_data_generator(x_val, batch_size=1)\n",
        "val_input, _ = next(val_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opW3JpCsPXPS"
      },
      "source": [
        "# registration of this validation pair\n",
        "val_pred = vxm_model.predict(val_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is72zRvfPXPW"
      },
      "source": [
        "# visualize registration\n",
        "images = [img[0, :, :, 0] for img in val_input + val_pred]\n",
        "titles = ['moving', 'fixed', 'moved', 'flow']\n",
        "ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io8onq5mPXPa"
      },
      "source": [
        "# visualize the flow\n",
        "flow = val_pred[1].squeeze()[::3,::3]\n",
        "ne.plot.flow([flow], width=5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1axrla6VPXPf"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSQj-OWwPXPg"
      },
      "source": [
        "Evaluating registration results is tricky. The first tendancy is to look at the images (as above), and conclude that if they match, the registration has succeeded.\n",
        "\n",
        "However, image matching can be achieved by an optimization that only penalizes the image matching term, and does not care whether the deformation field is reasonable. For example, next we compare our model with one that was trained on maximizing MSE only (without smoothness loss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSo3Ec3PPXPg"
      },
      "source": [
        "# prediction from model with MSE + smoothness loss\n",
        "vxm_model.load_weights('brain_2d_smooth.h5')\n",
        "our_val_pred = vxm_model.predict(val_input)\n",
        "\n",
        "# prediction from model with just MSE loss\n",
        "vxm_model.load_weights('brain_2d_no_smooth.h5')\n",
        "mse_val_pred = vxm_model.predict(val_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxEXwAJrPXPi",
        "scrolled": false
      },
      "source": [
        "def visualize_vxm_results(vols, title):\n",
        "  images = [img[0, ..., 0] for img in vols]\n",
        "  mse = tf.keras.metrics.mean_squared_error(images[0], images[1]).numpy().mean()\n",
        "  titles = ['fixed', '%s (mse: %.5f)' % (title, mse), 'flow']\n",
        "  ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);\n",
        "\n",
        "# visualize MSE + smoothness model output\n",
        "visualize_vxm_results([val_input[1], *our_val_pred], 'mse + smooth')\n",
        "\n",
        "# visualize MSE model output\n",
        "visualize_vxm_results([val_input[1], *mse_val_pred], 'mse only')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk_hrf9xsqJU"
      },
      "source": [
        "The MSE is (usually) lower for the second network. So the images 'match up better' in terms of intensity alignment. But the fields to achieve this are much less reasonable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlxvpuoGPXPk"
      },
      "source": [
        "flows = [img[1].squeeze()[::3, ::3] for img in [our_val_pred, mse_val_pred]]\n",
        "titles = ['mse + smooth', 'mse only']\n",
        "ne.plot.flow(flows, width=10, titles=titles);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTx7PeIGPXPm"
      },
      "source": [
        "What we often do to evaluate the registration is to use **external anotations**. One variant is using anatomical segmentations.  \n",
        "\n",
        "In the next section, we demonstrate the use of a 3D model, and show how to evaluate it with segmentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZxdnWCvPXPo"
      },
      "source": [
        "# 3D MRI brain scan registration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1D471jXPXPo"
      },
      "source": [
        "Finally we get to 3D models, which are of particular interest in medical image analysis.\n",
        "\n",
        "However, due to the size of the models and data, we won't be able to train a model within a short tutorial time. Instead, here we assume one has been trained, and demonstrate its use. You can train one very similar to how you trained the 2D models above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03eUI2GjPXPo"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpkI4qC9PXPp"
      },
      "source": [
        "# our data will be of shape 160 x 192 x 224\n",
        "vol_shape = (160, 192, 224)\n",
        "nb_features = [\n",
        "    [16, 32, 32, 32],\n",
        "    [32, 32, 32, 32, 32, 16, 16]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMZfiO1vPXPt"
      },
      "source": [
        "# build vxm network\n",
        "vxm_model = vxm.networks.VxmDense(vol_shape, nb_features, int_steps=0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXZzOD2juQ8x"
      },
      "source": [
        "Load a trained 3D model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QsxSz11uNrF"
      },
      "source": [
        "vxm_model.load_weights('brain_3d.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve9jX06fPXPu"
      },
      "source": [
        "### Validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8fQOjC0jjsI"
      },
      "source": [
        "Let's prepare our validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsXgWlS3PXPv"
      },
      "source": [
        "val_volume_1 = np.load('subj1.npz')['vol']\n",
        "seg_volume_1 = np.load('subj1.npz')['seg']\n",
        "val_volume_2 = np.load('subj2.npz')['vol']\n",
        "seg_volume_2 = np.load('subj2.npz')['seg']\n",
        "\n",
        "val_input = [\n",
        "    val_volume_1[np.newaxis, ..., np.newaxis],\n",
        "    val_volume_2[np.newaxis, ..., np.newaxis]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIGFUGxyPXPy"
      },
      "source": [
        "## Registration\n",
        "Now let's register two 3d validation volumes. The first time the model is run it will take a bit extra time due to some behind the scenes initializations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXHBPty3PXPz"
      },
      "source": [
        "val_pred = vxm_model.predict(val_input);\n",
        "moved_pred = val_pred[0].squeeze() # moved volume.\n",
        "pred_warp = val_pred[1]            # warp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmXDAMAfvnQV"
      },
      "source": [
        "## Visualize Results\n",
        "To visualize, we'll need to take out slices, since these are 3D volumes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEVuVrqbPXP2"
      },
      "source": [
        "# Some useful functions first\n",
        "def extract_slices(vol):\n",
        "  mid_slices = [np.take(vol, vol.shape[d]//1.8, axis=d) for d in range(3)]\n",
        "  mid_slices[1] = np.rot90(mid_slices[1], 1)\n",
        "  mid_slices[2] = np.rot90(mid_slices[2], -1)\n",
        "  return mid_slices\n",
        "\n",
        "titlefn = lambda x: ['%s %s' % (x, f) for f in ['saggital', 'axial', 'coronal']]\n",
        "\n",
        "# show moving, fixed, and moved volumes\n",
        "ne.plot.slices(extract_slices(val_volume_1), titles=titlefn('moving'), cmaps=['gray'], width=10)\n",
        "ne.plot.slices(extract_slices(val_volume_2), titles=titlefn('fixed'), cmaps=['gray'], width=10)\n",
        "ne.plot.slices(extract_slices(moved_pred), titles=titlefn('moved'), cmaps=['gray'], width=10);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtsXTkS4PXP5"
      },
      "source": [
        "Let's look at the segmentations! To do this, we'll need to warp segmentation maps.\n",
        "\n",
        "We'll need to grab a model that does such warping first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUVJDZsyPXP6"
      },
      "source": [
        "# let's get a model that transforms images with nearest-enighbout interpolation.\n",
        "# this model takes in a volume and a deformation fields, and outputs the deformed volume\n",
        "warp_model = vxm.networks.Transform(vol_shape, interp_method='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APSI8UyUPXP7"
      },
      "source": [
        "# let's warp the 'moving' segmentaiton map\n",
        "warped_seg = warp_model.predict([seg_volume_1[np.newaxis,...,np.newaxis], pred_warp])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBX-6YP1PXP-"
      },
      "source": [
        "To prepare out visualization, we're first going to prepare a colormap. We use the freesurfer colormap, which has been carefully chosen to clearly see distinct brain structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rfi9i0sPXP_"
      },
      "source": [
        "import matplotlib\n",
        "\n",
        "fs_colors = np.load('fs_rgb.npy')\n",
        "ccmap = matplotlib.colors.ListedColormap(fs_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JrTnlCpPXQA"
      },
      "source": [
        "Let's visualize the segmentation maps, and make sure they are sensible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKtm_fUAPXQB"
      },
      "source": [
        "mid_slices_moving = seg_volume_1.squeeze()[int(seg_volume_1.shape[0]//1.8), ...]\n",
        "mid_slices_fixed = seg_volume_2.squeeze()[int(seg_volume_1.shape[0]//1.8), ...]\n",
        "mid_slices_moved = warped_seg.squeeze()[int(seg_volume_1.shape[0]//1.8), ...]\n",
        "\n",
        "slices = [mid_slices_moving, mid_slices_fixed, mid_slices_moved]\n",
        "\n",
        "titles = ['moving', 'fixed', 'moved']\n",
        "# the imshow arguments here are simply to maintain freesurfer colors\n",
        "ne.plot.slices(slices, cmaps=[ccmap], imshow_args=[{'vmin':0, 'vmax':255}], titles=titles);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QN9ceYXr9tk"
      },
      "source": [
        "We're only visualizing a saggital slice here, for ease of visualization. We can see that the anatomical regions match up substantially better after registration (i.e. between moved and fixed)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YsOcys-PXQD"
      },
      "source": [
        "## Runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6lz_zpXPXQD"
      },
      "source": [
        "An important advantage of learning-based methods is the dramatically lowered runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9bnd5kwPXQD"
      },
      "source": [
        "%timeit vxm_model.predict(val_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsD9LD5CPXQG"
      },
      "source": [
        "In our tests, a run is 10s, for an entire 3D volume. Classically, this would take tens of minutes to hours on a CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-1bZxgrtGc9"
      },
      "source": [
        "---------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYidisbHeYZO"
      },
      "source": [
        "# Bonus: Atlas Building Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iO7dq43K_zA"
      },
      "source": [
        "We'll cover one advanced topic (if all of the above makes sense!) with a bit less focus on the details and more focus on the big picture of new models: atlas (template) building.\n",
        "\n",
        "Atlases (templates) are powerful analysis entities that are often behind the current workhorse piplines in medical image analysis. There are very popular atlases that are available for download (like MNI brain atlas), but we often want to build a template that represents our data. Here, we'll walk through how to [build templates within VoxelMorph directly](https://arxiv.org/abs/1908.02738).\n",
        "\n",
        "First, we'll have to do some book keeping again: some installations and data wrangling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awBRZLC9Vbwh"
      },
      "source": [
        "!pip install tensorflow_addons  # for tqdm callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzBWfnYCPO1a"
      },
      "source": [
        "# some imports we'll need throughout the demo\n",
        "import os\n",
        "\n",
        "# some third party very useful libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa  # for TQDM callback\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import nibabel as nib\n",
        "\n",
        "# our libraries\n",
        "import voxelmorph as vxm\n",
        "import neurite as ne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2J071cQptvx"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZsrgMyNpth0"
      },
      "source": [
        "# turn off eager for this\n",
        "# need to do it due to some tf 2.0+ compatibility issues\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1lwekV6pLYz"
      },
      "source": [
        "# some helpful functions\n",
        "def plot_hist(hist):\n",
        "  plt.figure(figsize=(17,5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(hist.epoch, hist.history['loss'], '.-')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epochs');\n",
        "  plt.grid()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  nb_epochs = len(hist.epoch) // 2\n",
        "  plt.plot(hist.epoch[-nb_epochs:], hist.history['loss'][-nb_epochs:], '.-')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epochs');\n",
        "  plt.grid()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Ml_e6kDOe8"
      },
      "source": [
        "# generally useful callback\n",
        "# unfortunately show_epoch_progress=True leaves a printout that we can't control (bad implementation in tfa...)\n",
        "tqdm_cb = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False, show_epoch_progress=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ygb0ZFVDteQ"
      },
      "source": [
        "### Data\n",
        "We'll need MNIST again with slightly different processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylmIFe6WpV29"
      },
      "source": [
        "# let's load up MNIST again\n",
        "(x_train_all, y_train_all), (x_test_all, y_test_all) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "x_train_all = x_train_all.astype('float')/255\n",
        "x_test_all = x_test_all.astype('float')/255\n",
        "\n",
        "x_train_all = np.pad(x_train_all, ((0, 0), (2, 2), (2, 2)), 'constant')[..., np.newaxis]\n",
        "x_test_all = np.pad(x_test_all, ((0, 0), (2, 2), (2, 2)), 'constant')[..., np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsY8_Y0ZPS3P"
      },
      "source": [
        "# extract all 5s\n",
        "digit = 5\n",
        "\n",
        "x_train = x_train_all[y_train_all == digit, ...]\n",
        "y_train = y_train_all[y_train_all == digit]\n",
        "x_test = x_test_all[y_test_all == digit, ...].astype('float')/255\n",
        "y_test = y_test_all[y_test_all == digit]\n",
        "\n",
        "vol_shape = list(x_train.shape[1:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBS7qK45digx"
      },
      "source": [
        "# prepare a simple generator to template generation\n",
        "def template_gen(x, batch_size):\n",
        "  vol_shape = list(x.shape[1:-1])\n",
        "  zero = np.zeros([batch_size] + vol_shape + [2])\n",
        "  mean_atlas = np.repeat(np.mean(x, 0, keepdims=True), batch_size, 0)\n",
        "\n",
        "  while True:\n",
        "    idx = np.random.randint(0, x.shape[0], batch_size)\n",
        "    img = x[idx, ...]\n",
        "    inputs = [mean_atlas, img]\n",
        "    outputs = [img, zero, zero, zero]\n",
        "    yield inputs, outputs\n",
        "\n",
        "# let's make sure the sizes make sense\n",
        "sample = next(template_gen(x_train, 8))\n",
        "[f.shape for f in sample[0]], [f.shape for f in sample[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADanmU8xde-N"
      },
      "source": [
        "## Unconditional Template (MNIST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdIF0pxYSivj"
      },
      "source": [
        "We will build two types of templates: unconditional (one template per population) and conditional (i.e. a template as a function of attributes we care about, like diagnosis or age)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expFKVpnds54"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFdN2q_BUDJ8"
      },
      "source": [
        "The template creation is essentially the same as a voxelmorph model, but instead of the moving image being a simple image, it's a set of parameters at each voxel that will be learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxf85UqIdFI5"
      },
      "source": [
        "enc_nf = [16, 32, 32, 32]\n",
        "dec_nf = [32, 32, 32, 32, 32, 16, 16]\n",
        "model = vxm.networks.TemplateCreation(vol_shape, nb_unet_features=[enc_nf, dec_nf])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL-7FLurULtV"
      },
      "source": [
        "Because the model is so similar to voxelmorph, the losses will be as well. THere are two loss terms:\n",
        " - one *inverse similarity*, which is the similarity of applying the inverse of the deformation field to the image, to move it to the atlas\n",
        " - one *centrality* that encourages the **average** of all deformation fields in the dataset to be zero, making the atlas *central*  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGhdphmgQaFo"
      },
      "source": [
        "# prepare losses and compile\n",
        "image_loss_func = vxm.losses.MSE().loss\n",
        "neg_loss_func = lambda _, y_pred: image_loss_func(model.references.atlas_tensor, y_pred)\n",
        "losses = [image_loss_func, neg_loss_func, vxm.losses.MSE().loss, vxm.losses.Grad('l2', loss_mult=2).loss]\n",
        "loss_weights = [0.5, 0.5, 1, 0.01]\n",
        "\n",
        "model.compile('adam', loss=losses, loss_weights=loss_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXy6KLvrSKbc"
      },
      "source": [
        "# train model\n",
        "gen = template_gen(x_train, batch_size=8)\n",
        "hist = model.fit(gen, epochs=100, steps_per_epoch=25, verbose=0, callbacks=[tqdm_cb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg-XSIhMesyj"
      },
      "source": [
        "# visualize training curve\n",
        "plot_hist(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHWHPruPhN1Y"
      },
      "source": [
        "Okay, this seems somewhat converged. For now, we will focus on analyzing what the atlas looks like after this training. But remember, this is a full voxelmorph model as well -- meaning it can register the images (in this case, 5s) to the atlas!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt-CPXfdD46S"
      },
      "source": [
        "### Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EynNvFplSeUP"
      },
      "source": [
        "# visualize learned atlas\n",
        "atlas = model.references.atlas_layer.get_weights()[0][..., 0]\n",
        "plt.imshow(atlas, cmap='gray')\n",
        "plt.axis('off');\n",
        "plt.title('atlas')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVOcyZH_hbpq"
      },
      "source": [
        "Looks like an average but sharp 5! Substantially sharper than average all 5s, or any other simplistic method.\n",
        "\n",
        "Okay, so what does this buy us? This enables learning central templates (atlases) very quickly, which we often need for analyses.\n",
        "\n",
        "But it also enables easy extensions that can be very powerful, like conditional templates, which we cover below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-t3ztxZgpW1"
      },
      "source": [
        "## Unconditional Template (2D Brain slices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHyB-9QOiSRb"
      },
      "source": [
        "Let's try the same thing with brains (in this case, we'll keep to 2D mid-coronal slices, to make the tutorial work)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUu1VjrdD6_N"
      },
      "source": [
        "### Get Brain Data\n",
        "This is data we released as part of neurite, please read more about it [here](https://github.com/adalca/medical-datasets/blob/master/neurite-oasis.md)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57q6VXORhg7b"
      },
      "source": [
        "# get the data\n",
        "!wget wget http://surfer.nmr.mgh.harvard.edu/ftp/data/neurite/data/neurite-oasis.2d.v1.0.tar -O data.tar\n",
        "!tar -xf data.tar;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gddK5393h5bM"
      },
      "source": [
        "# prepare data\n",
        "files = [f + '/slice_norm.nii.gz' for f in os.listdir('.') if f.startswith('OASIS_OAS1_')]\n",
        "vols = [nib.load(f).get_fdata() for f in tqdm(files)]\n",
        "x_vols = np.stack(vols, 0)\n",
        "vol_shape = x_vols.shape[1:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY-xqHMzEF_J"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wXokSB7KimLh"
      },
      "source": [
        "# get the model\n",
        "enc_nf = [16, 32, 32, 32]\n",
        "dec_nf = [32, 32, 32, 32, 32, 16, 16]\n",
        "model = vxm.networks.TemplateCreation(vol_shape, nb_unet_features=[enc_nf, dec_nf])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ic1uHcGji5O6"
      },
      "source": [
        "# prepare losses\n",
        "image_loss_func = vxm.losses.MSE().loss\n",
        "neg_loss_func = lambda _, y_pred: image_loss_func(model.references.atlas_tensor, y_pred)\n",
        "losses = [image_loss_func, neg_loss_func, vxm.losses.MSE().loss, vxm.losses.Grad('l2', loss_mult=2).loss]\n",
        "loss_weights = [0.5, 0.5, 1, 0.01]\n",
        "\n",
        "model.compile('adam', loss=losses, loss_weights=loss_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U67yiOM-i6uW"
      },
      "source": [
        "# train\n",
        "gen = template_gen(x_vols, batch_size=2)\n",
        "hist = model.fit(gen, epochs=100, steps_per_epoch=25, verbose=0, callbacks=[tqdm_cb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lGIhkpYIjKkx"
      },
      "source": [
        "# visualize optimization\n",
        "plot_hist(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AhA4oRWESAI"
      },
      "source": [
        "### Visualize Atlas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9OylH0vLjBNz"
      },
      "source": [
        "atlas = model.references.atlas_layer.get_weights()[0][..., 0]\n",
        "plt.imshow(np.rot90(atlas, -1), cmap='gray')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghjP_ZGBie7d"
      },
      "source": [
        "This simple template code was able to capture important structures in the brain while still being central and representing the 'average brain'. There are a varity of ways to analyze this brain -- how central it is, how sharp it is, how well other scans can register to it, how good/bad those registrations are (which you can get with the same trained model!), etc. These are beyond the scope of this tutorial, but interesting and active research directions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICiteHXdrf6R"
      },
      "source": [
        "## Conditional Template (MNIST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6n8H1WHi05Q"
      },
      "source": [
        "Finally, we describe how to build *conditional* templates -- template that are not just one image, but a *function* of attributes we care about. In medical imaging, important attributes we often care about are age or genetic makeup.\n",
        "\n",
        "For this tutorial, we will work with MNIST as an example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6bCKmBVEU9F"
      },
      "source": [
        "## Data (all MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8KEw6EfreSA"
      },
      "source": [
        "# back to MNIST, all digits this time\n",
        "x_train = x_train_all\n",
        "y_train = y_train_all\n",
        "y_train_onehot = tf.keras.utils.to_categorical(y_train_all, 10)\n",
        "x_test = x_test_all\n",
        "y_test = y_train_all\n",
        "vol_shape = list(x_train.shape[1:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUWMZihir87g"
      },
      "source": [
        "# prepare a simple generator.\n",
        "def cond_template_gen(x, y, batch_size):\n",
        "  vol_shape = list(x.shape[1:-1])\n",
        "  zero = np.zeros([batch_size] + vol_shape + [2])\n",
        "  atlas = np.repeat(np.mean(x, 0, keepdims=True), batch_size, 0)\n",
        "\n",
        "  while True:\n",
        "    idx = np.random.randint(0, x.shape[0], batch_size)\n",
        "    img = x[idx, ...]\n",
        "    inputs = [y[idx, ...], atlas, img]\n",
        "\n",
        "    outputs = [img, zero, zero, zero]\n",
        "    yield inputs, outputs\n",
        "\n",
        "sample = next(cond_template_gen(x_train, y_train_onehot, 8))\n",
        "[f.shape for f in sample[0]], [f.shape for f in sample[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoXa0BCOEZ9P"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbWK9CfLjPJ_"
      },
      "source": [
        "The model is a simple extension to the previous template model -- but instead of learning one template, there's a small decoder architecture -- which takes in teh attribute, and outputs the template. This in turn gets fed as the 'moving' image to a voxelmorph architecture, and it's all trained end to end. During training, we feed in one attribute and one image -- so for example, we feed in (5, X) where X is a MNIST image of the digit 5.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kybjrJWOyGRP"
      },
      "source": [
        "enc_nf = [16,32,32,32]\n",
        "dec_nf = [32,32,32,32,16,16,3]\n",
        "model = vxm.networks.ConditionalTemplateCreation(vol_shape,\n",
        "                                                 pheno_input_shape=[10], # 10 digits\n",
        "                                                 nb_unet_features=[enc_nf, dec_nf],\n",
        "                                                 conv_nb_features=16,\n",
        "                                                 conv_image_shape=[4, 4, 8],\n",
        "                                                 conv_nb_levels=4)\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uz-MhPvj3Vy"
      },
      "source": [
        "The losses are basically the same as before! Which is great, fewer changes are better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC-g__CMyrNY"
      },
      "source": [
        "# prepare losses\n",
        "image_loss_func = vxm.losses.MSE().loss\n",
        "losses = [image_loss_func, vxm.losses.MSE().loss, vxm.losses.Grad('l2', loss_mult=2).loss, vxm.losses.MSE().loss]\n",
        "loss_weights = [1, 0.01, 0.03, 0]  # changed second-last to 0.01\n",
        "\n",
        "model.compile('adam', loss=losses, loss_weights=loss_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up_-WfXRytBP"
      },
      "source": [
        "# fit the model. This one might take a bit longer as it's a bit more involved -\n",
        "# but feel free to change the epochs or steps to be smaller.\n",
        "# It will give less converged results, but likely still illustrate the ideas.\n",
        "gen = cond_template_gen(x_train, y_train_onehot, batch_size=32)\n",
        "hist = model.fit(gen, epochs=100, steps_per_epoch=10, verbose=0, callbacks=[tqdm_cb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCJ7VHa20Ap7"
      },
      "source": [
        "plot_hist(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMn8olPSEhyc"
      },
      "source": [
        "## Visualize atlas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK-pIFtMj9pM"
      },
      "source": [
        "Finally, let's visualize the result. Since this is a conditional atlas, we will extract the decoder (atlas) model, which takes in attributes and outputs the appropriate atlas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tJcqNHN1qNq"
      },
      "source": [
        "atlas_model = tf.keras.models.Model(model.inputs[:2], model.get_layer('atlas').output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg5nSY_tkPqR"
      },
      "source": [
        "Prepare the input data (basically a list of the digits we want to see)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkHtBIkk1-8R"
      },
      "source": [
        "mean_atlas = np.repeat(np.mean(x_train, 0, keepdims=True), 10, 0)\n",
        "input_samples = [tf.keras.utils.to_categorical(np.arange(10), 10), mean_atlas]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfXFl1P4kUxq"
      },
      "source": [
        "Get the atlases by running the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9RxTOeu2idR"
      },
      "source": [
        "pred = atlas_model.predict(input_samples)\n",
        "ne.plot.slices([f.squeeze() for f in pred], cmaps=['gray']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5EMkXnCl_uk"
      },
      "source": [
        "Neat! We can see templates learned as a condition of the input attribute (the digit) purely from the data. Of course, these are not converged yet, so they will get sharper as the model is trained more.\n",
        "\n",
        "More examples can be found [here](http://voxelmorph.mit.edu/atlas_creation/) -- be sure to go use this on your own data, and hopefully this tutorial has enabled you to build your own models!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oseJR9CzPXMb"
      },
      "source": [
        "# References\n",
        "[VoxelMorph at TMI](https://arxiv.org/abs/1809.05231)   \n",
        "[Diffeomorphic VoxelMorph at MedIA](https://arxiv.org/abs/1903.03545)   \n",
        "[Neurite Library](https://github.com/adalca/neuron) - [CVPR](http://arxiv.org/abs/1903.03148)  \n",
        "[Template Building - NeurIPS](https://arxiv.org/abs/1908.02738)"
      ]
    }
  ]
}