{
"nbformat": 4,
"nbformat_minor": 0,
"metadata": {
"accelerator": "GPU",
"kernelspec": {
"display_name": "Python 3",
"name": "python3"
},
"language_info": {
"name": "python",
"version": "3.x"
}
},
"cells": [
{
"cell_type": "markdown",
"source": [
"# MECエントリー：VoxelMorphフレームワークによる画像レジストレーション入門\n",
"Adrian Dalca and Andrew Hoopes\n",
"\n",
"このチュートリアルでは、VoxelMorphフレームワークを用いた、教師なし学習ベースの画像レジストレーションについて解説します。\n",
"\n",
"### 概要\n",
"- MNISTで学ぶコアコンセプト\n",
" まず、データの扱い方、モデルの構築、学習、レジストレーション、そして汎化について学びます。\n",
"- より現実的な複雑さ：脳MRI（2Dスライス）\n",
" 次に、これらのモデルが2Dの脳スキャン画像に対してどのように機能するかを示し、より複雑なシナリオを提示します。\n",
"- 実践的な3D脳MRI\n",
" 完全な3Dレジストレーションの例を説明します。\n",
"- ボーナス：テンプレート（アトラス）の構築\n",
"\n",
"このチュートリアルはTensorFlowに焦点を当てていますが、ほとんどの機能に対応するPyTorchコードも利用可能です。"
]
},
{
"cell_type": "markdown",
"source": [
"このチュートリアル全体を通して、画像は既に剛体変換によって（おおよそ）同様の空間に位置合わせされていることを前提とします。\n",
"剛体位置合わせもVoxelMorphで可能ですが、ここでは扱いません。"
]
},
{
"cell_type": "markdown",
"source": [
"---"
]
},
{
"cell_type": "markdown",
"source": [
"# 準備"
]
},
{
"cell_type": "markdown",
"source": [
"## 環境設定"
]
},
{
"cell_type": "code",
"source": [
"# voxelmorphをインストールします。依存関係にあるneuriteとpystrumも同時にインストールされます。\n",
"!pip install voxelmorph -q"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"まず、共通して使用するライブラリをインポートします。"
]
},
{
"cell_type": "code",
"source": [
"# 必要なライブラリをインポートします\n",
"import os, sys\n",
"\n",
"# サードパーティのライブラリをインポートします\n",
"import numpy as np\n",
"import tensorflow as tf\n",
"assert tf.version.startswith('2.'), 'このチュートリアルはTensorflow 2.0以上を想定しています。'"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"次に、私たちを助けてくれる2つのパッケージをインポートします。\n",
"- voxelmorph は、ディープラーニングベースのレジストレーションライブラリです。\n",
"- neurite は、TensorFlowを用いた医用画像解析のためのライブラリです。"
]
},
{
"cell_type": "code",
"source": [
"# ローカルライブラリをインポートします\n",
"import voxelmorph as vxm\n",
"import neurite as ne"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"---"
]
},
{
"cell_type": "markdown",
"source": [
"## データ"
]
},
{
"cell_type": "markdown",
"source": [
"まず2DのMNIST数字画像のレジストレーションから始め、その後、医用画像データへと進みます。データが小さい場合（2DのMNISTのように）、メモリにロードすることができます。これにより、学習とテストが高速になります。データが大きい場合（巨大な3Dスキャンなど）、スキャンデータをその都度ロードする必要があるでしょう。これについては後で詳しく説明します。\n",
"\n",
"まず、データをロードします。幸いなことに、MNISTはKerasフレームワークに付属しているので、ここで簡単にロードできます。\n"
]
},
{
"cell_type": "code",
"source": [
"# このインポート文は通常、他のすべてのインポート文と一緒に先頭に書くべきですが、\n",
"# ここではデータの出所を明示するために、あえてこの位置に記述します。\n",
"from tensorflow.keras.datasets import mnist"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# MNISTデータをロードします。\n",
"# mnist.load_data()は、データを学習用とテスト用に自動で分割してくれます。\n",
"(x_train_load, y_train_load), (x_test_load, y_test_load) = mnist.load_data()\n",
"\n",
"# レジストレーション対象の数字を選択します\n",
"digit_sel = 5\n",
"\n",
"# 数字が「5」のインスタンスのみを抽出します\n",
"x_train = x_train_load[y_train_load==digit_sel, ...]\n",
"y_train = y_train_load[y_train_load==digit_sel]\n",
"x_test = x_test_load[y_test_load==digit_sel, ...]\n",
"y_test = y_test_load[y_test_load==digit_sel]\n",
"\n",
"# ロードしたデータの形状を確認し、理解を深めます\n",
"print('x_trainの形状: {}, y_trainの形状: {}'.format(x_train.shape, y_train.shape))"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"機械学習の寄り道: データを学習用/テスト用だけに分けることは、しばしば問題を引き起こします。\n",
"(A)モデルを構築し、(B)学習データで学習させ、(C)テストデータで評価する、というサイクルを繰り返すべきではありません。\n",
"そうすると、テストセットに過学習（オーバーフィット）してしまいます（なぜなら、アルゴリズムをテストデータに合わせて調整してしまったからです）。これは機械学習の論文投稿でよくある間違いです。\n",
"\n",
"ここでは「学習用」データをさらに「学習用/検証用」データに分割し、テストセットは後にとっておきます。\n",
"そして、テストデータは（論文を投稿する準備が整った）最後の最後まで見ないようにします。"
]
},
{
"cell_type": "code",
"source": [
"nb_val = 1000 # 1,000個のデータを検証用に確保します\n",
"x_val = x_train[-nb_val:, ...] # このインデックス指定は、0番目の軸の「最後のnb_val個の要素」を意味します\n",
"y_val = y_train[-nb_val:]\n",
"x_train = x_train[:-nb_val, ...]\n",
"y_train = y_train[:-nb_val]"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"### データの可視化"
]
},
{
"cell_type": "markdown",
"source": [
"データのロードが終わったら、必ずデータを可視化することが重要です。\n",
"ここでは、neuriteというパッケージのツールを使います。これは内部でmatplotlibを使用しています。\n",
"直接matplotlibを使うこともできますが、少しコードが煩雑になります。\n",
"ここでは主要なコンセプトを説明することに集中します。"
]
},
{
"cell_type": "code",
"source": [
"nb_vis = 5\n",
"\n",
"# nb_vis個のサンプルインデックスをランダムに選びます\n",
"idx = np.random.choice(x_train.shape, nb_vis, replace=False)\n",
"example_digits = [f for f in x_train[idx, ...]]\n",
"\n",
"# プロットします\n",
"ne.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"良さそうですね！\n",
"\n",
"しかし、幸いにもカラーバーを表示させたことで、データの範囲がであることがわかりました。\n",
"ニューラルネットワークでは、や[-1, 1]といった範囲で作業するのが一般的です。\n",
"これを修正しましょう。\n",
"\n",
"一般的に、常にカラーバー付きでデータをプロットすべきです。これにより、学習を始める前に問題を発見できます。"
]
},
{
"cell_type": "code",
"source": [
"# データを修正します\n",
"x_train = x_train.astype('float')/255\n",
"x_val = x_val.astype('float')/255\n",
"x_test = x_test.astype('float')/255\n",
"\n",
"# 確認します\n",
"print('学習データの最大値:', x_train.max())"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# 再度可視化します\n",
"example_digits = [f for f in x_train[idx, ...]]\n",
"ne.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"最後にもう一つ変更を加えます。後ほど、多くの一般的なモデルが、層の数をNとして2^Nの倍数のサイズの入力を好むことを見ます。ここでは、画像のサイズを32（= 2 * 2^4）に強制します。"
]
},
{
"cell_type": "code",
"source": [
"pad_amount = ((0, 0), (2,2), (2,2))\n",
"\n",
"# データを修正（パディング）します\n",
"x_train = np.pad(x_train, pad_amount, 'constant')\n",
"x_val = np.pad(x_val, pad_amount, 'constant')\n",
"x_test = np.pad(x_test, pad_amount, 'constant')\n",
"\n",
"# 確認します\n",
"print('学習データの形状:', x_train.shape)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"---"
]
},
{
"cell_type": "markdown",
"source": [
"# 教師なし画像レジストレーション (VoxelMorph)"
]
},
{
"cell_type": "markdown",
"source": [
"voxelmorphで教師なしレジストレーションを行うには、2つのものが必要です。1つはCNNモデル（2つの画像を入力とし、私たちが必要とする変形場を出力するもの）、もう1つは学習を可能にする損失関数です。"
]
},
{
"cell_type": "markdown",
"source": [
"## CNNモデル"
]
},
{
"cell_type": "markdown",
"source": [
"2つの画像（moving画像とfixed画像と呼びます）が与えられたとき、私たちの目標はそれらの間の変形を見つけることです。学習ベースの手法では、2つの画像 
𝑚
m
（"moving"）と 
𝑓
f
（"fixed"）（例：32x32サイズのMNIST数字）を入力とし、密な変形 
𝑝
ℎ
𝑖
phi
（例：サイズ32x32x2、各ピクセルでどこへ移動すべきかを示すベクトル）を出力するネットワークを使用します。直感的には、この変形 
𝑝
ℎ
𝑖
phi
 は画像間の対応関係を与え、moving画像をfixed画像に一致させるためにどのように動かせばよいかを教えてくれます。\n",
"\n",
"注: レジストレーションにはアフィン変換も含まれます（またはそれを指すこともあります）が、このチュートリアルでは無視します。\n",
"\n",
"VoxelMorphライブラリは、密な変形ネットワークを構築するためのVxmDenseモデルクラスを提供しています。このクラスについては後で説明しますが、このチュートリアルでは、ネットワークの個々のコンポーネントを示すために、まずこのモデルをゼロから構築することから始めます。最初に、vxm.networks.Unet()モデルでUNetアーキテクチャを抽象化します。"
]
},
{
"cell_type": "code",
"source": [
"# UNetの入力形状を設定します（moving画像とfixed画像の連結）\n",
"ndim = 2\n",
"unet_input_features = 2\n",
"inshape = (x_train.shape[1:], unet_input_features)\n",
"\n",
"# UNetの特徴マップ数を設定します\n",
"nb_features = [\n",
" , # エンコーダ側の特徴マップ数\n",
" # デコーダ側の特徴マップ数\n",
"]\n",
"\n",
"# モデルを構築します\n",
"unet = vxm.networks.Unet(inshape=inshape, nb_features=nb_features)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"このモデルを少し見てみましょう..."
]
},
{
"cell_type": "code",
"source": [
"print('入力形状: ', unet.input.shape)\n",
"print('出力形状:', unet.output.shape)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"次に、最終出力が各ボクセルでの変形を表す2つの特徴量を持つようにする必要があります。"
]
},
{
"cell_type": "code",
"source": [
"# 結果を変形場（flow field）に変換します\n",
"disp_tensor = tf.keras.layers.Conv2D(ndim, kernel_size=3, padding='same', name='disp')(unet.output)\n",
"\n",
"# テンソルの形状を確認します\n",
"print('変位（displacement）テンソル:', disp_tensor.shape)\n",
"\n",
"# Kerasを使えば、テンソルへのポインタを介して簡単に新しいモデルを形成できます\n",
"def_model = tf.keras.models.Model(unet.inputs, disp_tensor)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"変形モデルdef_modelは、UNetモデルとレイヤーを共有します。したがって、UNetのレイヤーの重みを変更すると、def_modelの重みも変更されます。"
]
},
{
"cell_type": "markdown",
"source": [
"## 損失関数"
]
},
{
"cell_type": "markdown",
"source": [
"変位
𝑝
ℎ
𝑖
phi
がネットワークから出力されるとして、\n",
"それが意味のあるものかどうかを判断するための損失を考える必要があります。"
]
},
{
"cell_type": "markdown",
"source": [
"教師ありの設定では、正解の変形 
𝑝
ℎ
𝑖
𝑔
𝑡
phi
gt
	​

 があり、\n",
"MSE 
=
∣
𝑝
ℎ
𝑖
−
𝑝
ℎ
𝑖
𝑔
𝑡
∣
=
∣
phi−
phi
gt
	​

∣
 のような教師あり損失を使用できます。"
]
},
{
"cell_type": "markdown",
"source": [
"教師なしレジストレーションの主なアイデアは、古典的なレジストレーションから着想を得た損失を使用することです。\n",
"\n",
"教師なしで、この変形が良いものであるとどうやってわかるのでしょうか？\n",
"(1) 
𝑚
𝑐
𝑖
𝑟
𝑐
𝑝
ℎ
𝑖
m
circ
phi
（
𝑝
ℎ
𝑖
phi
によってワープされた
𝑚
m
）が、何らかの類似性の尺度で
𝑓
f
に近くなるようにする。\n",
"(2) 
𝑝
ℎ
𝑖
phi
を正則化する（多くの場合、滑らかにすることを意味する）。"
]
},
{
"cell_type": "markdown",
"source": [
"(1)を達成するためには、入力画像
𝑚
m
をワープする必要があります。これを行うために、空間変換ネットワーク層を使用します。これは本質的に線形補間を行います。"
]
},
{
"cell_type": "code",
"source": [
"# 空間変換層を構築します\n",
"spatial_transformer = vxm.layers.SpatialTransformer(name='transformer')\n",
"\n",
"# UNetの入力テンソルから最初のフレーム（つまり「moving」画像）を抽出します\n",
"# 入力は [moving, fixed] の連結なので、最初のチャネルが moving 画像です\n",
"moving_image = tf.expand_dims(unet.input[..., 0], axis=-1)\n",
"\n",
"# 空間変換層でmoving画像をワープします\n",
"moved_image_tensor = spatial_transformer([moving_image, disp_tensor])"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"ワープされた画像が固定画像に近くなるようにし、(2)で
𝑝
ℎ
𝑖
phi
の滑らかさ損失を達成するために、これらのテンソルを損失関数に入れることができます。しかし、これらの2つのテンソルを完全なモデルからの出力として扱うと便利なことが多いです。そうすれば、テスト時にワープされた画像と変形場の両方を一度に取得できます。"
]
},
{
"cell_type": "code",
"source": [
"outputs = [moved_image_tensor, disp_tensor]\n",
"vxm_model_manual = tf.keras.models.Model(inputs=unet.inputs, outputs=outputs)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"今作成したモデルは、標準的なVoxelMorphの密なアーキテクチャを表しており、UNetコンポーネント、変位場、そして最終的な空間変換層で構成されています。しかし、毎回このモデルをゼロから構築する必要はありません。VoxelMorphライブラリは、このアーキテクチャを内包する、高度にカスタマイズ可能なVxmDenseモデルクラスを提供しています。\n",
"\n",
"チュートリアルのこれ以降では、VxmDenseクラスを使ってモデルを構築するので、同等のネットワークを再構築してみましょう。"
]
},
{
"cell_type": "code",
"source": [
"# VxmDenseを使用してモデルを構築します\n",
"inshape = x_train.shape[1:]\n",
"vxm_model = vxm.networks.VxmDense(inshape, nb_features, int_steps=0)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"int_steps=0オプションは、diffeomorphism（微分同相写像）を無効にします。これはチュートリアルのより高度なステップでカバーされます。\n",
"\n",
"VxmDenseモデルは、自動的に2つの入力テンソル（movingとfixed）を持つように構成されます（1つではなく）。入力形状パラメータには特徴量の情報を含めるべきではありません。特徴量情報は、オプションのsrc_featsとtrg_featsパラメータで設定できます。\n",
"\n",
"最後にモデルをもう一度見てみましょう。"
]
},
{
"cell_type": "code",
"source": [
"print('入力形状: ', ', '.join([str(t.shape) for t in vxm_model.inputs]))\n",
"print('出力形状:', ', '.join([str(t.shape) for t in vxm_model.outputs]))"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"ネットワークを簡単に構築する方法を学んだので、実際の損失を定義しましょう。Kerasの仕組みでは、各出力に対して損失項を定義する必要があります。これは、教師なしレジストレーションにおける2つの損失（画像マッチングと正則化）と一致します。\n",
"\n",
"最初の損失は簡単で、ワープされた画像
𝑚
𝑐
𝑖
𝑟
𝑐
𝑝
ℎ
𝑖
m
circ
phi
と固定画像
𝑓
f
の間のMSEのような類似度です。voxelmorphライブラリには、さまざまなカスタム損失クラスがあります。\n",
"\n",
"2番目の損失には、変位の空間勾配を使用します。\n",
"ここではこれをゼロからコーディングするのではなく、voxelmorphの実装を使用します。\n",
"\n",
"また、ハイパーパラメータで損失項のバランスをとる必要があります。これについては、HyperMorphという手法を通じて、発展セクションでより深く掘り下げます。"
]
},
{
"cell_type": "code",
"source": [
"# 損失項を定義します\n",
"# losses = [vxm.losses.MSE().loss, vxm.losses.Grad('l2').loss] # 古い書き方\n",
"losses = [vxm.losses.MSE, vxm.losses.Grad('l2')]\n",
"\n",
"# 通常、ハイパーパラメータで2つの損失のバランスをとる必要があります\n",
"lambda_param = 0.05\n",
"loss_weights = [1, lambda_param]"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"最後に、モデルをコンパイルします。\n",
"これにより、モデルに損失関数とオプティマイザを関連付け、学習の準備が整います。"
]
},
{
"cell_type": "code",
"source": [
"vxm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=losses, loss_weights=loss_weights)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"## モデルの学習"
]
},
{
"cell_type": "markdown",
"source": [
"学習を行うには、データが正しい形式であり、私たちが望む方法でモデルに供給されることを確認する必要があります。\n",
"\n",
"Kerasモデルはmodel.fitで学習できます。これは、すべてのデータが大きな配列にある場合や、データのバッチを供給するPythonジェネレータを介して機能します。\n",
"\n",
"MNISTデータに基づいた単純なデータジェネレータをコーディングしてみましょう。これにより、各イテレーションでデータがどのように供給されるかという核となる考え方が伝わるはずです。"
]
},
{
"cell_type": "code",
"source": [
"def vxm_data_generator(x_data, batch_size=32):\n",
" """\n",
" [N, H, W]のサイズのデータを入力とし、カスタムvxmモデル用のデータを出力するジェネレータ。\n",
" 各入力と各出力に対してnumpyデータを提供する必要があることに注意してください。\n",
"\n",
" 入力: moving画像 [bs, H, W, 1], fixed画像 [bs, H, W, 1]\n",
" 出力: (比較対象の)moved画像 [bs, H, W, 1], ゼロ勾配 [bs, H, W, 2]\n",
" """\n",
"\n",
" # 事前準備\n",
" vol_shape = x_data.shape[1:] # データ形状の抽出\n",
" nb_items = x_data.shape\n",
" ndims = len(vol_shape)\n",
"\n",
" # 変形場と同じサイズのゼロ配列を準備します\n",
" # これについては後で説明します\n",
" zero_phi = np.zeros([batch_size, vol_shape, ndims])\n",
"\n",
" while True:\n",
" # 入力を準備します:\n",
" # 画像は[batch_size, H, W, 1]のサイズである必要があります\n",
" idx1 = np.random.randint(0, nb_items, size=batch_size)\n",
" moving_images = x_data[idx1, ..., np.newaxis]\n",
" idx2 = np.random.randint(0, nb_items, size=batch_size)\n",
" fixed_images = x_data[idx2, ..., np.newaxis]\n",
" inputs = [moving_images, fixed_images]\n",
"\n",
" # 出力（「真の」moved画像）を準備します:\n",
" # もちろん、これ（真のmoved画像）は持っていませんが、結果のmoved画像を\n",
" # fixed画像と比較したいことはわかっています。そのため、fixed画像を返します。\n",
" # Kerasは各出力に対して「正解」項目を要求するため、\n",
" # 変形場の正解データとしてゼロのボリュームを返します。これは最終的には使用されません。\n",
" outputs = [fixed_images, zero_phi]\n",
"\n",
" yield (inputs, outputs)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# テストしてみましょう\n",
"train_generator = vxm_data_generator(x_train)\n",
"in_sample, out_sample = next(train_generator)\n",
"\n",
"# 可視化します\n",
"images = [img[0, :, :, 0] for img in in_sample + out_sample]\n",
"titles = ['moving', 'fixed', '"moved正解データ" (fixed)', 'zeros']\n",
"ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"最後に、モデルを学習させます。このチュートリアルでの学習速度は、Googleから割り当てられるCPU/GPU/TPUに依存するため、各ステップで数イテレーションだけ学習させます。\n",
"\n",
"通常は、収束するまで学習させます。つまり、レジストレーション損失が一定のイテレーション数で改善しなくなるまでです。"
]
},
{
"cell_type": "code",
"source": [
"# それでは、モデルを学習させましょう\n",
"nb_epochs = 10\n",
"steps_per_epoch = 100\n",
"hist = vxm_model.fit(train_generator, epochs=nb_epochs, steps_per_epoch=steps_per_epoch, verbose=2);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"数値を読み取るだけでなく、損失を可視化することは常に良い習慣です。これにより、収束したかどうかなどをよりよく理解できます。TensorFlowはTensorBoardという強力な対話型可視化システムを提供していますが、この短いチュートリアルでは、単純に損失をプロットします。"
]
},
{
"cell_type": "code",
"source": [
"import matplotlib.pyplot as plt\n",
"\n",
"def plot_history(hist, loss_name='loss'):\n",
" # 学習履歴をプロットする簡単な関数\n",
" plt.figure()\n",
" plt.plot(hist.epoch, hist.history[loss_name], '.-')\n",
" plt.ylabel('損失 (loss)')\n",
" plt.xlabel('エポック (epoch)')\n",
" plt.grid()\n",
" plt.show()\n",
"\n",
"plot_history(hist)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"明らかに、これは収束していません。通常は収束するまで実行すべきですが、このチュートリアルの目的のため、次に進みます。\n",
"\n",
"寄り道: 常に損失グラフを最後の数イテレーション/エポックにズームインしてください。グラフ全体を見ると、開始点の影響で最終エポックが平坦に見えることがありますが、実際にはそうでないことが多いです。"
]
},
{
"cell_type": "markdown",
"source": [
"## レジストレーションの実行"
]
},
{
"cell_type": "markdown",
"source": [
"ペアワイズ最適化手法（ほとんどの古典的手法など）では、新しいペアをレジストレーションするために変形場を最適化する必要があります。\n",
"\n",
"学習ベースのレジストレーションでは、新しい入力ペアに対してネットワークを評価するだけです。"
]
},
{
"cell_type": "code",
"source": [
"# 検証用データを取得します\n",
"val_generator = vxm_data_generator(x_val, batch_size=1)\n",
"val_input, _ = next(val_generator)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"レジストレーション: predict()は、本質的に入力が与えられたネットワークを実行します。"
]
},
{
"cell_type": "code",
"source": [
"val_pred = vxm_model.predict(val_input)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"これだけです！\n",
"\n",
"これはMNISTだけですが、どれくらいの時間がかかるか見てみましょう。"
]
},
{
"cell_type": "code",
"source": [
"# %timeitは、指定された行を数回実行して時間を計測する「Jupyterマジック」です\n",
"%timeit vxm_model.predict(val_input)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"MNISTであっても、これは非常に高速です。結果を可視化してみましょう。"
]
},
{
"cell_type": "code",
"source": [
"# 可視化します\n",
"images = [img[0, :, :, 0] for img in val_input + val_pred]\n",
"titles = ['moving', 'fixed', 'moved', 'flow (x-dim)']\n",
"# 変形場のx成分のみを可視化します\n",
"ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"変形場（flow）をもう少し分かりやすく可視化してみましょう。小さな矢印を描画します。\n"
]
},
{
"cell_type": "code",
"source": [
"# flowはneuriteのプロット機能です\n",
"ne.plot.flow([val_pred.squeeze()], width=5);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"## 汎化性能\n",
"学習ベースの手法は、学習データの分布を超えてどのように汎化するのでしょうか？"
]
},
{
"cell_type": "markdown",
"source": [
"学習ベースのレジストレーションの重要な注意点は、一般的に、学習された分布からのサンプルに対してのみレジストレーションを行うということです。\n",
"\n",
"では、2つの「7」をレジストレーションするとどうなるでしょうか？"
]
},
{
"cell_type": "code",
"source": [
"# 数字が「7」のインスタンスのみを抽出します\n",
"x_sevens = x_train_load[y_train_load==7, ...].astype('float') / 255\n",
"x_sevens = np.pad(x_sevens, pad_amount, 'constant')\n",
"\n",
"# 予測します\n",
"seven_generator = vxm_data_generator(x_sevens, batch_size=1)\n",
"seven_sample, _ = next(seven_generator)\n",
"seven_pred = vxm_model.predict(seven_sample)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# 可視化します\n",
"images = [img[0, :, :, 0] for img in seven_sample + seven_pred]\n",
"titles = ['moving', 'fixed', 'moved', 'flow (x-dim)']\n",
"ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"興味深いことに、まだ機能しています！つまり、私たちが予想した以上に汎化しました。なぜでしょうか？\n",
"\n",
"局所的に見ると、「7」の一部は「5」と似ているため、レジストレーションアルゴリズムはまだ局所的な近傍を一致させようとします。\n",
"\n",
"では、別のバリエーションを試してみましょう。元の（数字の5の）データセットの輝度値をある係数で乗算したらどうなるでしょうか？"
]
},
{
"cell_type": "code",
"source": [
"factor = 5\n",
"val_pred = vxm_model.predict([f * factor for f in val_input])\n",
"\n",
"# 可視化します\n",
"images = [img[0, :, :, 0] for img in [val_inputfactor, val_inputfactor] + val_pred]\n",
"titles = ['moving (x5)', 'fixed (x5)', 'moved', 'flow (x-dim)']\n",
"ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"これは破綻しました！なぜでしょうか？この場合、ネットワークは（輝度値が非常に大きいため）この画像の一部すら見たことがありません。その場合、ネットワークが何をするかはわかりません。うまくいくかもしれないし、いかないかもしれません。\n",
"\n",
"ネットワークがいつ汎化し、いつしないのかを理解することは非常に重要であり、今も活発な研究分野です。"
]
},
{
"cell_type": "markdown",
"source": [
"# 脳MRIの教師なしレジストレーション"
]
},
{
"cell_type": "markdown",
"source": [
"次に、もう少し現実的なデータ、脳のMRIをレジストレーションします。\n",
"\n",
"このチュートリアル中に簡単に学習とレジストレーションができるように、まず脳スキャンの中央スライスを抽出します。このタスクは3次元目の変形を捉えないため、特定の対応付けは厳密には不可能です。それでも、この演習はより現実的で複雑な画像でのレジストレーションを説明するのに役立ちます。\n",
"\n",
"脳スキャンは、変形可能なレジストレーションに焦点を当てるために、FreeSurferを用いて輝度正規化、アフィン位置合わせ、頭蓋骨除去が行われています。これらのステップは厳密には必要ないかもしれませんが、チュートリアルを進める上で役立ちます。"
]
},
{
"cell_type": "markdown",
"source": [
"## データ：脳スキャン"
]
},
{
"cell_type": "code",
"source": [
"# MRIチュートリアルデータをダウンロードします\n",
"!wget https://surfer.nmr.mgh.harvard.edu/pub/data/voxelmorph/tutorial_data.tar.gz -O data.tar.gz -q\n",
"!tar -xzvf data.tar.gz > /dev/null"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# このチュートリアルのためにパッケージ化されたデータをロードします\n",
"npz = np.load('tutorial_data.npz')\n",
"x_train = npz['train']\n",
"x_val = npz['validate']\n",
"\n",
"# 208個のボリュームは192x160のサイズです\n",
"print('学習データの形状:', x_train.shape)\n",
"vol_shape = x_train.shape[1:]"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"### データの可視化\n",
"データの一部を見てみましょう。"
]
},
{
"cell_type": "code",
"source": [
"# いくつかの脳画像を抽出します\n",
"nb_vis = 5\n",
"idx = np.random.randint(0, x_train.shape, [5,])\n",
"example_brains = [f for f in x_train[idx, ...]]\n",
"\n",
"# 可視化します\n",
"ne.plot.slices(example_brains, cmaps=['gray'], do_colorbars=True);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"## モデル"
]
},
{
"cell_type": "markdown",
"source": [
"MNISTと同様に、MSEと空間平滑化損失で学習された標準的なVoxelMorphモデルを作成しましょう。"
]
},
{
"cell_type": "code",
"source": [
"# UNetの特徴マップ数を設定します（MNISTと同じ設定を再利用します）\n",
"nb_features = [\n",
" ,\n",
" \n",
"]\n",
"\n",
"# VoxelMorphモデルを構築します\n",
"vxm_model = vxm.networks.VxmDense(vol_shape, nb_features, int_steps=0)\n",
"\n",
"# 損失と損失の重みを設定します\n",
"# losses = ['mse', vxm.losses.Grad('l2').loss] # 古い書き方\n",
"losses = [vxm.losses.MSE, vxm.losses.Grad('l2')]\n",
"loss_weights = [1, 0.01]"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"実験から、この問題および小さなバッチサイズに対しては、Adamオプティマイザの学習率を1e-3ではなく1e-4にするとパフォーマンスが向上することがわかっています。"
]
},
{
"cell_type": "code",
"source": [
"vxm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=losses, loss_weights=loss_weights)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"幸いなことに、以前と同じデータジェネレータを使用できます。変更されたのはジェネレータに渡すデータのスタックだけだからです。\n",
"\n",
"まずテストしてみましょう。"
]
},
{
"cell_type": "code",
"source": [
"# 新しいジェネレータを取得します\n",
"train_generator = vxm_data_generator(x_train, batch_size=8)\n",
"in_sample, out_sample = next(train_generator)\n",
"\n",
"# 可視化します\n",
"images = [img[0, :, :, 0] for img in in_sample + out_sample]\n",
"titles = ['moving', 'fixed', '"moved正解データ" (fixed)', 'zeros']\n",
"ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"良さそうです。モデルを学習する時間です。\n",
"先ほどと同様に、説明のために少しだけ学習させます。皆さんは完了するまで学習させてください！"
]
},
{
"cell_type": "code",
"source": [
"nb_epochs = 5\n",
"steps_per_epoch = 20\n",
"hist = vxm_model.fit(train_generator, epochs=nb_epochs, steps_per_epoch=steps_per_epoch, verbose=2);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# 先ほどと同様に、何が起こったか可視化しましょう\n",
"plot_history(hist)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"チュートリアルの目的のため、非常に少ないエポック数しか実行していません。もちろん、ここで何らかの合理的な収束が期待できるわけではありません。\n",
"\n",
"あまり長く待たずにチュートリアルを進めるために、200エポック事前学習させたモデルをロードしましょう。"
]
},
{
"cell_type": "code",
"source": [
"# 事前学習済みモデルの重みをダウンロードします\n",
"!wget https://surfer.nmr.mgh.harvard.edu/pub/data/voxelmorph/brain_2d_smooth.h5 -q\n",
"# 重みをロードします\n",
"vxm_model.load_weights('brain_2d_smooth.h5')"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"それでは、検証データを使っていくつかの結果を見てみましょう。"
]
},
{
"cell_type": "code",
"source": [
"# 検証データジェネレータを作成します\n",
"val_generator = vxm_data_generator(x_val, batch_size=1)\n",
"val_input, _ = next(val_generator)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# この検証ペアのレジストレーションを実行します\n",
"val_pred = vxm_model.predict(val_input)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# レジストレーションを可視化します\n",
"images = [img[0, :, :, 0] for img in val_input + val_pred]\n",
"titles = ['moving', 'fixed', 'moved', 'flow (x-dim)']\n",
"ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# 変形場（flow）を可視化します\n",
"# 密すぎるので3ピクセルごとに間引いて表示します\n",
"flow = val_pred.squeeze()[::3,::3]\n",
"ne.plot.flow([flow], width=5);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"## 評価"
]
},
{
"cell_type": "markdown",
"source": [
"レジストレーション結果の評価は厄介です。最初の傾向として、画像を見て（上記のように）、それらが一致していればレジストレーションは成功したと結論付けがちです。\n",
"\n",
"しかし、画像の類似度だけを評価し、変形場が妥当であるかどうかを気にしない最適化によって、画像の一致は達成できます。例えば、次に私たちのモデルと、MSEのみを最大化するように学習されたモデル（平滑化損失なし）とを比較します。"
]
},
{
"cell_type": "code",
"source": [
"# MSE + 平滑化損失を持つモデルからの予測\n",
"vxm_model.load_weights('brain_2d_smooth.h5')\n",
"our_val_pred = vxm_model.predict(val_input)\n",
"\n",
"# MSE損失のみを持つモデルからの予測\n",
"!wget https://surfer.nmr.mgh.harvard.edu/pub/data/voxelmorph/brain_2d_no_smooth.h5 -q\n",
"vxm_model_no_smooth = vxm.networks.VxmDense(vol_shape, nb_features, int_steps=0)\n",
"vxm_model_no_smooth.load_weights('brain_2d_no_smooth.h5')\n",
"mse_val_pred = vxm_model_no_smooth.predict(val_input)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"def visualize_vxm_results(fixed, moved, flow, title):\n",
" images = [fixed, moved, flow[..., 0]]\n",
" mse = tf.keras.metrics.mean_squared_error(fixed, moved).numpy().mean()\n",
" titles = ['fixed', '%s (mse: %.5f)' % (title, mse), 'flow (x-dim)']\n",
" ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);\n",
"\n",
"# MSE + 平滑化モデルの出力を可視化\n",
"visualize_vxm_results(val_input[0,...,0], our_val_pred[0,...,0], our_val_pred[0,...], 'mse + smooth')\n",
"\n",
"# MSEのみモデルの出力を可視化\n",
"visualize_vxm_results(val_input[0,...,0], mse_val_pred[0,...,0], mse_val_pred[0,...], 'mse only')"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"MSEは（通常）2番目のネットワークの方が低いです。つまり、輝度の一致という点では、画像は「より良く一致」しています。しかし、これを達成するための変形場は、はるかに妥当ではありません。"
]
},
{
"cell_type": "code",
"source": [
"flows = [img.squeeze()[::3, ::3] for img in [our_val_pred, mse_val_pred]]\n",
"titles = ['mse + smooth', 'mse only']\n",
"ne.plot.flow(flows, width=10, titles=titles);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"レジストレーションを評価するために私たちがよく行うことは、外部のアノテーションを使用することです。その一つに、解剖学的セグメンテーションを使用する方法があります。\n",
"\n",
"次のセクションでは、3Dモデルの使用法を示し、セグメンテーションでそれを評価する方法を説明します。"
]
},
{
"cell_type": "markdown",
"source": [
"# 3D MRI脳スキャンレジストレーション"
]
},
{
"cell_type": "markdown",
"source": [
"最後に、医用画像解析で特に興味深い3Dモデルに取り組みます。\n",
"\n",
"しかし、モデルとデータのサイズのため、短いチュートリアル時間内にモデルを学習させることはできません。代わりに、ここではモデルが学習済みであると仮定し、その使用法を示します。上記の2Dモデルを学習させたのと非常によく似た方法で3Dモデルを学習させることができます。"
]
},
{
"cell_type": "markdown",
"source": [
"### モデル"
]
},
{
"cell_type": "code",
"source": [
"# データは160 x 192 x 224の形状になります\n",
"vol_shape = (160, 192, 224)\n",
"nb_features = [\n",
" , # エンコーダ\n",
" # デコーダ\n",
"]"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# vxmネットワークを構築します\n",
"vxm_model = vxm.networks.VxmDense(vol_shape, nb_features, int_steps=0);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"学習済みの3Dモデルをロードします。"
]
},
{
"cell_type": "code",
"source": [
"!wget https://surfer.nmr.mgh.harvard.edu/pub/data/voxelmorph/brain_3d.h5 -q\n",
"vxm_model.load_weights('brain_3d.h5')"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"### 検証データ"
]
},
{
"cell_type": "markdown",
"source": [
"検証データを準備しましょう。"
]
},
{
"cell_type": "code",
"source": [
"!wget https://surfer.nmr.mgh.harvard.edu/pub/data/voxelmorph/subj1.npz -q\n",
"!wget https://surfer.nmr.mgh.harvard.edu/pub/data/voxelmorph/subj2.npz -q\n",
"\n",
"val_volume_1 = np.load('subj1.npz')['vol']\n",
"seg_volume_1 = np.load('subj1.npz')['seg']\n",
"val_volume_2 = np.load('subj2.npz')['vol']\n",
"seg_volume_2 = np.load('subj2.npz')['seg']\n",
"\n",
"val_input = [\n",
" val_volume_1[np.newaxis, ..., np.newaxis],\n",
" val_volume_2[np.newaxis, ..., np.newaxis]\n",
"]"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"## レジストレーション\n",
"それでは、2つの3D検証ボリュームをレジストレーションしてみましょう。モデルを初めて実行するときは、舞台裏での初期化のため、少し余分な時間がかかります。"
]
},
{
"cell_type": "code",
"source": [
"val_pred = vxm_model.predict(val_input);\n",
"moved_pred = val_pred.squeeze() # ワープされたボリューム\n",
"pred_warp = val_pred # 変形場"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"## 結果の可視化\n",
"これらは3Dボリュームなので、可視化するためにはスライスを抽出する必要があります。"
]
},
{
"cell_type": "code",
"source": [
"# まず、いくつかの便利な関数を定義します\n",
"def extract_slices(vol):\n",
" # 3つの軸（矢状断、軸位断、冠状断）の中心付近のスライスを抽出します\n",
" mid_slices = [np.take(vol, vol.shape[d]//1.8, axis=d) for d in range(3)]\n",
" mid_slices = np.rot90(mid_slices, 1)\n",
" mid_slices = np.rot90(mid_slices, -1)\n",
" return mid_slices\n",
"\n",
"titlefn = lambda x: ['%s %s' % (x, f) for f in ['矢状断(saggital)', '軸位断(axial)', '冠状断(coronal)']]\n",
"\n",
"# moving, fixed, moved ボリュームを表示します\n",
"ne.plot.slices(extract_slices(val_volume_1), titles=titlefn('moving'), cmaps=['gray'], width=10)\n",
"ne.plot.slices(extract_slices(val_volume_2), titles=titlefn('fixed'), cmaps=['gray'], width=10)\n",
"ne.plot.slices(extract_slices(moved_pred), titles=titlefn('moved'), cmaps=['gray'], width=10);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"セグメンテーションを見てみましょう！これを行うには、セグメンテーションマップをワープする必要があります。\n",
"\n",
"まず、そのようなワープを行うモデルを取得する必要があります。"
]
},
{
"cell_type": "code",
"source": [
"# 最近傍補間法で画像を変換するモデルを取得します。\n",
"# このモデルは、ボリュームと変形場を入力とし、変形したボリュームを出力します。\n",
"# セグメンテーションのようなラベルデータには最近傍補間が適しています。\n",
"warp_model = vxm.networks.Transform(vol_shape, interp_method='nearest')"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# 「moving」セグメンテーションマップをワープしましょう\n",
"warped_seg = warp_model.predict([seg_volume_1[np.newaxis,...,np.newaxis], pred_warp])"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"可視化の準備として、まずカラーマップを準備します。ここではFreeSurferのカラーマップを使用します。これは、異なる脳構造を明確に見るために慎重に選ばれています。"
]
},
{
"cell_type": "code",
"source": [
"import matplotlib\n",
"\n",
"!wget https://surfer.nmr.mgh.harvard.edu/pub/data/voxelmorph/fs_rgb.npy -q\n",
"fs_colors = np.load('fs_rgb.npy')\n",
"ccmap = matplotlib.colors.ListedColormap(fs_colors/255.0)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"セグメンテーションマップを可視化し、それらが妥当であることを確認しましょう。"
]
},
{
"cell_type": "code",
"source": [
"mid_slice_ax = seg_volume_1.shape//1.8\n",
"\n",
"mid_slices_moving = seg_volume_1.squeeze()[int(mid_slice_ax), ...]\n",
"mid_slices_fixed = seg_volume_2.squeeze()[int(mid_slice_ax), ...]\n",
"mid_slices_moved = warped_seg.squeeze()[int(mid_slice_ax), ...]\n",
"\n",
"slices = [mid_slices_moving, mid_slices_fixed, mid_slices_moved]\n",
"\n",
"titles = ['moving', 'fixed', 'moved']\n",
"# ここでのimshow引数は、FreeSurferの色を維持するためだけのものです\n",
"ne.plot.slices(slices, cmaps=[ccmap], imshow_args=[{'vmin':0, 'vmax':255}], titles=titles);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"ここでは可視化を容易にするため、矢状断スライスのみを可視化しています。レジストレーション後（movedとfixedの間）に、解剖学的領域が大幅によく一致していることがわかります。"
]
},
{
"cell_type": "markdown",
"source": [
"## 実行時間"
]
},
{
"cell_type": "markdown",
"source": [
"学習ベースの手法の重要な利点は、実行時間が劇的に短縮されることです。"
]
},
{
"cell_type": "code",
"source": [
"%timeit vxm_model.predict(val_input)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"私たちのテストでは、1回の実行は3Dボリューム全体で数秒です。古典的な手法では、CPUで数十分から数時間かかるでしょう。"
]
},
{
"cell_type": "markdown",
"source": [
"---"
]
},
{
"cell_type": "markdown",
"source": [
"# ボーナス：アトラス作成デモ"
]
},
{
"cell_type": "markdown",
"source": [
"（もし上記のすべてが理解できたなら！）詳細にはあまりこだわらず、新しいモデルの全体像に焦点を当てて、1つの高度なトピック、アトラス（テンプレート）構築について解説します。\n",
"\n",
"アトラス（テンプレート）は、現在の医用画像解析の主力パイプラインの背後にあることが多い強力な解析エンティティです。ダウンロード可能な非常に人気のあるアトラス（MNI脳アトラスなど）がありますが、私たちのデータを表すテンプレートを構築したい場合がよくあります。ここでは、VoxelMorph内で直接テンプレートを構築する方法を順を追って説明します。\n",
"\n",
"注記: このセクションでは、TensorFlow 1.xとの互換性のためにEager Executionを無効にするコードが含まれていましたが、最新のライブラリでは不要、あるいはエラーの原因となるため削除しています。代わりに、モデルのコンパイル時にjit_compile=Trueオプションを追加することで、パフォーマンスを向上させることができます。"
]
},
{
"cell_type": "code",
"source": [
"# このデモ全体で必要となるライブラリをインポートします\n",
"import os\n",
"import numpy as np\n",
"import matplotlib.pyplot as plt\n",
"import nibabel as nib\n",
"import tensorflow as tf\n",
"\n",
"# ライブラリをインポートします\n",
"import voxelmorph as vxm\n",
"import neurite as ne"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"### ユーティリティ"
]
},
{
"cell_type": "code",
"source": [
"# 役立つ関数\n",
"def plot_hist(hist):\n",
" plt.figure(figsize=(17, 5))\n",
" plt.subplot(1, 2, 1)\n",
" plt.plot(hist.epoch, hist.history['loss'], '.-')\n",
" plt.ylabel('損失 (loss)')\n",
" plt.xlabel('エポック (epochs)')\n",
" plt.grid()\n",
" plt.title('全期間の損失')\n",
"\n",
" plt.subplot(1, 2, 2)\n",
" nb_epochs = len(hist.epoch) // 2\n",
" plt.plot(hist.epoch[-nb_epochs:], hist.history['loss'][-nb_epochs:], '.-')\n",
" plt.ylabel('損失 (loss)')\n",
" plt.xlabel('エポック (epochs)')\n",
" plt.grid()\n",
" plt.title('後半の損失')\n",
" plt.show()"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"### データ\n",
"少し異なる処理を施したMNISTが再び必要になります。"
]
},
{
"cell_type": "code",
"source": [
"# MNISTを再度ロードします\n",
"(x_train_all, y_train_all), (x_test_all, y_test_all) = tf.keras.datasets.mnist.load_data()\n",
"x_train_all = x_train_all.astype('float')/255\n",
"x_test_all = x_test_all.astype('float')/255\n",
"\n",
"# パディングとチャネル次元の追加を行います\n",
"x_train_all = np.pad(x_train_all, ((0, 0), (2, 2), (2, 2)), 'constant')[..., np.newaxis]\n",
"x_test_all = np.pad(x_test_all, ((0, 0), (2, 2), (2, 2)), 'constant')[..., np.newaxis]"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# 数字の「5」をすべて抽出します\n",
"digit = 5\n",
"\n",
"x_train = x_train_all[y_train_all == digit, ...]\n",
"y_train = y_train_all[y_train_all == digit]\n",
"x_test = x_test_all[y_test_all == digit, ...]\n",
"y_test = y_test_all[y_test_all == digit]\n",
"\n",
"vol_shape = list(x_train.shape[1:-1])"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# テンプレート生成用の簡単なジェネレータを準備します\n",
"def template_gen(x, batch_size):\n",
" vol_shape = list(x.train.shape[1:-1])\n",
" zero = np.zeros([batch_size, vol_shape, 2])\n",
" # このデモでは、平均画像をアトラスの初期値として使用します\n",
" mean_atlas = np.repeat(np.mean(x, 0, keepdims=True), batch_size, 0)\n",
"\n",
" while True:\n",
" idx = np.random.randint(0, x.shape, batch_size)\n",
" img = x[idx, ...]\n",
" # 入力は [平均アトラス、現在の画像]\n",
" inputs = [mean_atlas, img]\n",
" # 出力は [現在の画像、ゼロ、ゼロ、ゼロ]\n",
" # 最後の3つは損失計算のターゲットです\n",
" outputs = [img, zero, zero, zero]\n",
" yield inputs, outputs\n"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"## 無条件テンプレート (MNIST)"
]
},
{
"cell_type": "markdown",
"source": [
"2種類のテンプレートを構築します：無条件テンプレート（集団ごとに1つのテンプレート）と条件付きテンプレート（診断や年齢など、関心のある属性の関数としてのテンプレート）です。"
]
},
{
"cell_type": "markdown",
"source": [
"### モデル"
]
},
{
"cell_type": "markdown",
"source": [
"テンプレート作成は本質的にVoxelMorphモデルと同じですが、moving画像が単純な画像である代わりに、学習される各ボクセルでのパラメータのセットになります。"
]
},
{
"cell_type": "code",
"source": [
"enc_nf =\n",
"dec_nf =\n",
"model = vxm.networks.TemplateCreation(vol_shape, nb_unet_features=[enc_nf, dec_nf])"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"モデルはVoxelMorphと非常に似ているため、損失も同様になります。損失項は2つあります：\n",
" - 1つは逆類似度で、これは変形場の逆を画像に適用してアトラスに移動させる際の類似度です。\n",
" - もう1つは中心性で、データセット内のすべての変形場の平均がゼロになるように促し、アトラスを中心的なものにします。"
]
},
{
"cell_type": "code",
"source": [
"# 損失を準備し、コンパイルします\n",
"image_loss_func = vxm.losses.MSE.loss\n",
"neg_loss_func = lambda _, y_pred: image_loss_func(model.references.atlas_tensor, y_pred)\n",
"losses = [image_loss_func, neg_loss_func, vxm.losses.MSE.loss, vxm.losses.Grad('l2', loss_mult=2).loss]\n",
"loss_weights = [0.5, 0.5, 1, 0.01]\n",
"\n",
"# jit_compile=True を使うと、特にグラフモードで学習が高速化されることがあります\n",
"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=losses, loss_weights=loss_weights, jit_compile=True)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# モデルを学習させます\n",
"gen = template_gen(x_train, batch_size=8)\n",
"# verbose=1でプログレスバーを表示します\n",
"hist = model.fit(gen, epochs=100, steps_per_epoch=25, verbose=1)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# 学習曲線を可視化します\n",
"plot_hist(hist)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"さて、これはある程度収束したようです。今のところ、この学習後のアトラスがどのように見えるかを分析することに焦点を当てます。しかし、覚えておいてください、これは完全なVoxelMorphモデルでもあります。つまり、画像（この場合は数字の「5」）をアトラスにレジストレーションすることができます！"
]
},
{
"cell_type": "markdown",
"source": [
"### 結果の可視化"
]
},
{
"cell_type": "code",
"source": [
"# 学習されたアトラスを可視化します\n",
"atlas = model.references.atlas_layer.get_weights()[..., 0]\n",
"plt.imshow(atlas, cmap='gray')\n",
"plt.axis('off');\n",
"plt.title('学習されたアトラス')"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"平均的でありながらシャープな「5」のように見えます！すべての「5」を平均したり、他の単純な手法よりも大幅にシャープです。\n",
"\n",
"さて、これは私たちに何をもたらすのでしょうか？これにより、解析にしばしば必要となる中心的なテンプレート（アトラス）を非常に迅速に学習できます。\n",
"\n",
"しかし、それはまた、以下で取り上げる条件付きテンプレートのような、非常に強力な簡単な拡張を可能にします。"
]
},
{
"cell_type": "markdown",
"source": [
"## 条件付きテンプレート (MNIST)"
]
},
{
"cell_type": "markdown",
"source": [
"最後に、条件付きテンプレートを構築する方法を説明します。これは単一の画像ではなく、関心のある属性の関数*としてのテンプレートです。医用画像では、私たちがしばしば関心を持つ重要な属性は、年齢や遺伝的構成などです。\n",
"\n",
"このチュートリアルでは、例としてMNISTを使用します。"
]
},
{
"cell_type": "markdown",
"source": [
"### データ (すべてのMNIST数字)"
]
},
{
"cell_type": "code",
"source": [
"# MNISTに戻り、今回はすべての数字を使います\n",
"x_train = x_train_all\n",
"y_train = y_train_all\n",
"y_train_onehot = tf.keras.utils.to_categorical(y_train_all, 10)\n",
"x_test = x_test_all\n",
"y_test = y_test_all\n",
"vol_shape = list(x_train.shape[1:-1])"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# 簡単なジェネレータを準備します。\n",
"def cond_template_gen(x, y, batch_size):\n",
" vol_shape = list(x.shape[1:-1])\n",
" zero = np.zeros([batch_size, *vol_shape, 2])\n",
" atlas = np.repeat(np.mean(x, 0, keepdims=True), batch_size, 0)\n",
"\n",
" while True:\n",
" idx = np.random.randint(0, x.shape, batch_size)\n",
" img = x[idx, ...]\n",
" # 入力は [条件(one-hot), 平均アトラス, 現在の画像]\n",
" inputs = [y[idx, ...], atlas, img]\n",
"\n",
" outputs = [img, zero, zero, zero]\n",
" yield inputs, outputs\n",
"\n",
"sample = next(cond_template_gen(x_train, y_train_onehot, 8))\n",
"[f.shape for f in sample], [f.shape for f in sample]"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"### モデル"
]
},
{
"cell_type": "markdown",
"source": [
"モデルは以前のテンプレートモデルの単純な拡張ですが、1つのテンプレートを学習する代わりに、小さなデコーダアーキテクチャがあります。これは属性を入力として受け取り、テンプレートを出力します。これが次にVoxelMorphアーキテクチャの「moving」画像として供給され、すべてがエンドツーエンドで学習されます。学習中、1つの属性と1つの画像を供給します。例えば、（「5」という属性、数字5のMNIST画像X）を供給します。"
]
},
{
"cell_type": "code",
"source": [
"enc_nf =\n",
"dec_nf =\n",
"model = vxm.networks.ConditionalTemplateCreation(vol_shape,\n",
" pheno_input_shape=, # 10個の数字\n",
" nb_unet_features=[enc_nf, dec_nf],\n",
" # 以下はアトラス生成デコーダのアーキテクチャです\n",
" atlas_feats=16,\n",
" atlas_shape=,\n",
" atlas_levels=4)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"損失は基本的に以前と同じです！変更が少ない方が良いですね！"
]
},
{
"cell_type": "code",
"source": [
"# 損失を準備します\n",
"image_loss_func = vxm.losses.MSE.loss\n",
"losses = [image_loss_func, vxm.losses.MSE.loss, vxm.losses.Grad('l2', loss_mult=2).loss, vxm.losses.MSE.loss]\n",
"loss_weights = [1, 0.01, 0.03, 0]\n",
"\n",
"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=losses, loss_weights=loss_weights, jit_compile=True)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"# モデルを学習させます。これは少し複雑なので、少し時間がかかるかもしれません。\n",
"# エポックやステップ数を小さくして自由に試してください。\n",
"# 収束は甘くなりますが、アイデアは示されるでしょう。\n",
"gen = cond_template_gen(x_train, y_train_onehot, batch_size=32)\n",
"hist = model.fit(gen, epochs=100, steps_per_epoch=25, verbose=1)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "code",
"source": [
"plot_hist(hist)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"## アトラスの可視化"
]
},
{
"cell_type": "markdown",
"source": [
"最後に、結果を可視化しましょう。これは条件付きアトラスなので、属性を入力として受け取り、適切なアトラスを出力するデコーダ（アトラス）モデルを抽出します。"
]
},
{
"cell_type": "code",
"source": [
"atlas_model = tf.keras.models.Model(model.inputs[:2], model.get_layer('atlas').output)"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"入力データ（基本的には見たい数字のリスト）を準備します。"
]
},
{
"cell_type": "code",
"source": [
"mean_atlas = np.repeat(np.mean(x_train, 0, keepdims=True), 10, 0)\n",
"input_samples = [tf.keras.utils.to_categorical(np.arange(10), 10), mean_atlas]"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"モデルを実行してアトラスを取得します。"
]
},
{
"cell_type": "code",
"source": [
"pred = atlas_model.predict(input_samples)\n",
"ne.plot.slices([f.squeeze() for f in pred], cmaps=['gray'], width=10, titles=[f'Digit {i}' for i in range(10)]);"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"source": [
"素晴らしい！入力属性（数字）を条件として、データから純粋に学習されたテンプレートを見ることができます。もちろん、これらはまだ収束していないので、モデルがさらに学習するにつれてシャープになります。\n",
"\n",
"より多くの例はこちらで見つけることができます。ぜひご自身のデータでこれを使ってみてください。このチュートリアルが、独自のモデルを構築する助けになれば幸いです！"
]
},
{
"cell_type": "markdown",
"source": [
"# 参考文献\n",
"VoxelMorph at TMI \n",
"Diffeomorphic VoxelMorph at MedIA \n",
"Neurite Library - CVPR \n",
"Template Building - NeurIPS"
]
}
]
}